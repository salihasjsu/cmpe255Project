{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "df = pd.read_csv('preprocessed_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOD_ID</th>\n",
       "      <th>NWCG_REPORTING_AGENCY</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIRE_SIZE_CLASS</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>OWNER_DESCR</th>\n",
       "      <th>STATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>CONT_DATE</th>\n",
       "      <th>CONT_TIME</th>\n",
       "      <th>COUNTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46487</td>\n",
       "      <td>FS</td>\n",
       "      <td>Salmon-Challis National Forest</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>45.308333</td>\n",
       "      <td>-114.475000</td>\n",
       "      <td>USFS</td>\n",
       "      <td>ID</td>\n",
       "      <td>1035</td>\n",
       "      <td>19920620</td>\n",
       "      <td>1336.0</td>\n",
       "      <td>Lemhi County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>242377</td>\n",
       "      <td>BLM</td>\n",
       "      <td>Rock Springs Field Office</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.416600</td>\n",
       "      <td>-109.234000</td>\n",
       "      <td>BLM</td>\n",
       "      <td>WY</td>\n",
       "      <td>1428</td>\n",
       "      <td>19920822</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Sweetwater County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49259</td>\n",
       "      <td>FS</td>\n",
       "      <td>Shasta-Trinity National Forest</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.730000</td>\n",
       "      <td>-122.475000</td>\n",
       "      <td>USFS</td>\n",
       "      <td>CA</td>\n",
       "      <td>1135</td>\n",
       "      <td>19920624</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Shasta County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50588</td>\n",
       "      <td>FS</td>\n",
       "      <td>Malheur National Forest</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>44.296667</td>\n",
       "      <td>-118.738333</td>\n",
       "      <td>USFS</td>\n",
       "      <td>OR</td>\n",
       "      <td>1538</td>\n",
       "      <td>19920801</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Grant County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222205</td>\n",
       "      <td>BLM</td>\n",
       "      <td>Susanville District</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.666600</td>\n",
       "      <td>-120.484400</td>\n",
       "      <td>BLM</td>\n",
       "      <td>CA</td>\n",
       "      <td>1100</td>\n",
       "      <td>19920628</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>Lassen County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376096</th>\n",
       "      <td>300286438</td>\n",
       "      <td>ST/C&amp;L</td>\n",
       "      <td>Arizona State Forestry Division - State Office</td>\n",
       "      <td>2015</td>\n",
       "      <td>20150316</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Missing/Undefined</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.493680</td>\n",
       "      <td>-112.277619</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1537</td>\n",
       "      <td>20150316</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Maricopa County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376097</th>\n",
       "      <td>300287009</td>\n",
       "      <td>ST/C&amp;L</td>\n",
       "      <td>San Mateo-Santa Cruz Unit</td>\n",
       "      <td>2015</td>\n",
       "      <td>20150423</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Missing/Undefined</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.414050</td>\n",
       "      <td>-122.193200</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>CA</td>\n",
       "      <td>1321</td>\n",
       "      <td>20150423</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>SAN MATEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376098</th>\n",
       "      <td>300307593</td>\n",
       "      <td>ST/C&amp;L</td>\n",
       "      <td>Butte Unit</td>\n",
       "      <td>2015</td>\n",
       "      <td>20150812</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Missing/Undefined</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.501869</td>\n",
       "      <td>-121.605211</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>CA</td>\n",
       "      <td>1246</td>\n",
       "      <td>20150812</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>BUTTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376099</th>\n",
       "      <td>300288242</td>\n",
       "      <td>ST/C&amp;L</td>\n",
       "      <td>Arizona State Forestry Division - State Office</td>\n",
       "      <td>2015</td>\n",
       "      <td>20150711</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Missing/Undefined</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.230100</td>\n",
       "      <td>-110.798700</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1382</td>\n",
       "      <td>20150711</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Pima County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376100</th>\n",
       "      <td>300285954</td>\n",
       "      <td>ST/C&amp;L</td>\n",
       "      <td>Arizona State Forestry Division - State Office</td>\n",
       "      <td>2015</td>\n",
       "      <td>20150125</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Missing/Undefined</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>34.255000</td>\n",
       "      <td>-109.692500</td>\n",
       "      <td>MISSING/NOT SPECIFIED</td>\n",
       "      <td>AZ</td>\n",
       "      <td>1255</td>\n",
       "      <td>20150125</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Apache County</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376101 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           FOD_ID NWCG_REPORTING_AGENCY  \\\n",
       "0           46487                    FS   \n",
       "1          242377                   BLM   \n",
       "2           49259                    FS   \n",
       "3           50588                    FS   \n",
       "4          222205                   BLM   \n",
       "...           ...                   ...   \n",
       "376096  300286438                ST/C&L   \n",
       "376097  300287009                ST/C&L   \n",
       "376098  300307593                ST/C&L   \n",
       "376099  300288242                ST/C&L   \n",
       "376100  300285954                ST/C&L   \n",
       "\n",
       "                            SOURCE_REPORTING_UNIT_NAME  FIRE_YEAR  \\\n",
       "0                       Salmon-Challis National Forest       1992   \n",
       "1                            Rock Springs Field Office       1992   \n",
       "2                       Shasta-Trinity National Forest       1992   \n",
       "3                              Malheur National Forest       1992   \n",
       "4                                  Susanville District       1992   \n",
       "...                                                ...        ...   \n",
       "376096  Arizona State Forestry Division - State Office       2015   \n",
       "376097                       San Mateo-Santa Cruz Unit       2015   \n",
       "376098                                      Butte Unit       2015   \n",
       "376099  Arizona State Forestry Division - State Office       2015   \n",
       "376100  Arizona State Forestry Division - State Office       2015   \n",
       "\n",
       "        DISCOVERY_DATE  STAT_CAUSE_CODE   STAT_CAUSE_DESCR  FIRE_SIZE  \\\n",
       "0             19920620              1.0          Lightning        0.1   \n",
       "1             19920821              1.0          Lightning        0.1   \n",
       "2             19920624              1.0          Lightning        0.1   \n",
       "3             19920801              1.0          Lightning        0.1   \n",
       "4             19920628              1.0          Lightning        0.1   \n",
       "...                ...              ...                ...        ...   \n",
       "376096        20150316             13.0  Missing/Undefined        0.1   \n",
       "376097        20150423             13.0  Missing/Undefined        1.0   \n",
       "376098        20150812             13.0  Missing/Undefined        0.1   \n",
       "376099        20150711             13.0  Missing/Undefined        0.1   \n",
       "376100        20150125             13.0  Missing/Undefined        0.1   \n",
       "\n",
       "        FIRE_SIZE_CLASS   LATITUDE   LONGITUDE            OWNER_DESCR STATE  \\\n",
       "0                     0  45.308333 -114.475000                   USFS    ID   \n",
       "1                     0  41.416600 -109.234000                    BLM    WY   \n",
       "2                     0  40.730000 -122.475000                   USFS    CA   \n",
       "3                     0  44.296667 -118.738333                   USFS    OR   \n",
       "4                     0  40.666600 -120.484400                    BLM    CA   \n",
       "...                 ...        ...         ...                    ...   ...   \n",
       "376096                0  33.493680 -112.277619  MISSING/NOT SPECIFIED    AZ   \n",
       "376097                1  37.414050 -122.193200  MISSING/NOT SPECIFIED    CA   \n",
       "376098                0  39.501869 -121.605211  MISSING/NOT SPECIFIED    CA   \n",
       "376099                0  32.230100 -110.798700  MISSING/NOT SPECIFIED    AZ   \n",
       "376100                0  34.255000 -109.692500  MISSING/NOT SPECIFIED    AZ   \n",
       "\n",
       "        DISCOVERY_TIME  CONT_DATE  CONT_TIME             COUNTY  \n",
       "0                 1035   19920620     1336.0       Lemhi County  \n",
       "1                 1428   19920822     1800.0  Sweetwater County  \n",
       "2                 1135   19920624     1800.0      Shasta County  \n",
       "3                 1538   19920801     2200.0       Grant County  \n",
       "4                 1100   19920628     1324.0      Lassen County  \n",
       "...                ...        ...        ...                ...  \n",
       "376096            1537   20150316     1800.0    Maricopa County  \n",
       "376097            1321   20150423     1334.0          SAN MATEO  \n",
       "376098            1246   20150812     1300.0              BUTTE  \n",
       "376099            1382   20150711     1800.0        Pima County  \n",
       "376100            1255   20150125     1800.0      Apache County  \n",
       "\n",
       "[376101 rows x 17 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "df[\"FIRE_SIZE_CLASS\"] = lb_make.fit_transform(df[\"FIRE_SIZE_CLASS\"]) # because it is ordinal\n",
    "#df[\"STAT_CAUSE_DESCR\"] = lb_make.fit_transform(df[\"STAT_CAUSE_DESCR\"])\n",
    "df['DISCOVERY_DATE'] = pd.to_datetime(df['DISCOVERY_DATE'])\n",
    "df['CONT_DATE'] = pd.to_datetime(df['CONT_DATE'])\n",
    "df['DISCOVERY_DATE']=df.DISCOVERY_DATE.apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "df['CONT_DATE'] = df.CONT_DATE.apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOD_ID</th>\n",
       "      <th>NWCG_REPORTING_AGENCY</th>\n",
       "      <th>SOURCE_REPORTING_UNIT_NAME</th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>STAT_CAUSE_CODE</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>FIRE_SIZE_CLASS</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>OWNER_DESCR</th>\n",
       "      <th>STATE</th>\n",
       "      <th>DISCOVERY_TIME</th>\n",
       "      <th>CONT_DATE</th>\n",
       "      <th>CONT_TIME</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>MONTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46487</td>\n",
       "      <td>FS</td>\n",
       "      <td>Salmon-Challis National Forest</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>45.308333</td>\n",
       "      <td>-114.475000</td>\n",
       "      <td>USFS</td>\n",
       "      <td>ID</td>\n",
       "      <td>1035</td>\n",
       "      <td>19920620</td>\n",
       "      <td>1336.0</td>\n",
       "      <td>Lemhi County</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>242377</td>\n",
       "      <td>BLM</td>\n",
       "      <td>Rock Springs Field Office</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.416600</td>\n",
       "      <td>-109.234000</td>\n",
       "      <td>BLM</td>\n",
       "      <td>WY</td>\n",
       "      <td>1428</td>\n",
       "      <td>19920822</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Sweetwater County</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49259</td>\n",
       "      <td>FS</td>\n",
       "      <td>Shasta-Trinity National Forest</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920624</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.730000</td>\n",
       "      <td>-122.475000</td>\n",
       "      <td>USFS</td>\n",
       "      <td>CA</td>\n",
       "      <td>1135</td>\n",
       "      <td>19920624</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>Shasta County</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50588</td>\n",
       "      <td>FS</td>\n",
       "      <td>Malheur National Forest</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>44.296667</td>\n",
       "      <td>-118.738333</td>\n",
       "      <td>USFS</td>\n",
       "      <td>OR</td>\n",
       "      <td>1538</td>\n",
       "      <td>19920801</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Grant County</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222205</td>\n",
       "      <td>BLM</td>\n",
       "      <td>Susanville District</td>\n",
       "      <td>1992</td>\n",
       "      <td>19920628</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>40.666600</td>\n",
       "      <td>-120.484400</td>\n",
       "      <td>BLM</td>\n",
       "      <td>CA</td>\n",
       "      <td>1100</td>\n",
       "      <td>19920628</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>Lassen County</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FOD_ID NWCG_REPORTING_AGENCY      SOURCE_REPORTING_UNIT_NAME  FIRE_YEAR  \\\n",
       "0   46487                    FS  Salmon-Challis National Forest       1992   \n",
       "1  242377                   BLM       Rock Springs Field Office       1992   \n",
       "2   49259                    FS  Shasta-Trinity National Forest       1992   \n",
       "3   50588                    FS         Malheur National Forest       1992   \n",
       "4  222205                   BLM             Susanville District       1992   \n",
       "\n",
       "   DISCOVERY_DATE  STAT_CAUSE_CODE STAT_CAUSE_DESCR  FIRE_SIZE  \\\n",
       "0        19920620              1.0        Lightning        0.1   \n",
       "1        19920821              1.0        Lightning        0.1   \n",
       "2        19920624              1.0        Lightning        0.1   \n",
       "3        19920801              1.0        Lightning        0.1   \n",
       "4        19920628              1.0        Lightning        0.1   \n",
       "\n",
       "   FIRE_SIZE_CLASS   LATITUDE   LONGITUDE OWNER_DESCR STATE  DISCOVERY_TIME  \\\n",
       "0                0  45.308333 -114.475000        USFS    ID            1035   \n",
       "1                0  41.416600 -109.234000         BLM    WY            1428   \n",
       "2                0  40.730000 -122.475000        USFS    CA            1135   \n",
       "3                0  44.296667 -118.738333        USFS    OR            1538   \n",
       "4                0  40.666600 -120.484400         BLM    CA            1100   \n",
       "\n",
       "   CONT_DATE  CONT_TIME             COUNTY  MONTH  \n",
       "0   19920620     1336.0       Lemhi County      1  \n",
       "1   19920822     1800.0  Sweetwater County      1  \n",
       "2   19920624     1800.0      Shasta County      1  \n",
       "3   19920801     2200.0       Grant County      1  \n",
       "4   19920628     1324.0      Lassen County      1  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MONTH'] = pd.DatetimeIndex(df['DISCOVERY_DATE']).month\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STAT_CAUSE_DESCR'] = df['STAT_CAUSE_DESCR'].apply(lambda x: 1 if x in ['Lightning'] else\n",
    "                                                     2 if x in ['Structure','Fireworks','Powerline','Railroad','Smoking','Children','Campfire','Equipment Use','Debris Burning']\n",
    "                                                         else 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['NWCG_REPORTING_AGENCY','FOD_ID','SOURCE_REPORTING_UNIT_NAME','FIRE_SIZE_CLASS',\n",
    "                'OWNER_DESCR','DISCOVERY_TIME','CONT_TIME','CONT_DATE','DISCOVERY_DATE','STAT_CAUSE_CODE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIRE_YEAR</th>\n",
       "      <th>STAT_CAUSE_DESCR</th>\n",
       "      <th>FIRE_SIZE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>MONTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>45.308333</td>\n",
       "      <td>-114.475</td>\n",
       "      <td>13</td>\n",
       "      <td>2144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>41.416600</td>\n",
       "      <td>-109.234</td>\n",
       "      <td>51</td>\n",
       "      <td>3561</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIRE_YEAR  STAT_CAUSE_DESCR  FIRE_SIZE   LATITUDE  LONGITUDE  STATE  \\\n",
       "0       1992                 0        0.1  45.308333   -114.475     13   \n",
       "1       1992                 0        0.1  41.416600   -109.234     51   \n",
       "\n",
       "   COUNTY  MONTH  \n",
       "0    2144      1  \n",
       "1    3561      1  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding State to labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb_make = LabelEncoder()\n",
    "df[\"STATE\"] = lb_make.fit_transform(df[\"STATE\"])\n",
    "df[\"COUNTY\"] = lb_make.fit_transform(df[\"COUNTY\"])\n",
    "df[\"STAT_CAUSE_DESCR\"] = lb_make.fit_transform(df[\"STAT_CAUSE_DESCR\"])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='STAT_CAUSE_DESCR')\n",
    "y = df['STAT_CAUSE_DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(y[:, np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,shuffle=True,random_state=30,stratify=y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(X_train_scaled,columns=X.columns.tolist())\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(X_test_scaled,columns=X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.iloc[:].values # converting into numpy array\n",
    "X_test = X_test.iloc[:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--smoke-test\", action=\"store_true\", help=\"Finish quickly for testing\")\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "def train_wildfire(config):\n",
    "    # https://github.com/tensorflow/tensorflow/issues/32159\n",
    "    import tensorflow as tf\n",
    "    batch_size = 1000\n",
    "    num_classes = 3\n",
    "    epochs = 200\n",
    "\n",
    "   # (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "   # x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(7,)),\n",
    "        tf.keras.layers.Dense(config[\"hidden\"], activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_classes)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            lr=config[\"lr\"]),\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[TuneReportCallback({\n",
    "            \"mean_accuracy\": \"accuracy\"\n",
    "        })])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-12 12:02:33,908\tINFO services.py:1090 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267\u001b[39m\u001b[22m\n",
      "2020-11-12 12:02:36,175\tWARNING worker.py:1091 -- Warning: The actor ImplicitFunc has size 30092420 when pickled. It will be stored in Redis, which could cause memory issues. This may mean that its definition uses a large array or other object.\n",
      "2020-11-12 12:02:36,247\tWARNING util.py:137 -- The `start_trial` operation took 0.5195848941802979 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: None<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 1/3 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=92583)\u001b[0m 2020-11-12 12:02:39.302561: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=92583)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=92583)\u001b[0m 2020-11-12 12:02:39.320058: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd41a01f740 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=92583)\u001b[0m 2020-11-12 12:02:39.320096: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=92581)\u001b[0m 2020-11-12 12:02:39.847935: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=92581)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=92581)\u001b[0m 2020-11-12 12:02:39.860719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f9e8074dcb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=92581)\u001b[0m 2020-11-12 12:02:39.860758: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-02-40\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.1488766223192215\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 1.37876296043396\n",
      "  time_this_iter_s: 1.37876296043396\n",
      "  time_total_s: 1.37876296043396\n",
      "  timestamp: 1605168160\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-02-41\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.14855091273784637\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 1.46806001663208\n",
      "  time_this_iter_s: 1.46806001663208\n",
      "  time_total_s: 1.46806001663208\n",
      "  timestamp: 1605168161\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: None<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.86516</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148551</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.46806</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-02-46\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 12\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 6.879186153411865\n",
      "  time_this_iter_s: 0.5241241455078125\n",
      "  time_total_s: 6.879186153411865\n",
      "  timestamp: 1605168166\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 12\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: None<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         7.38589</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         6.42803</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-02-46\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 12\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 6.933888912200928\n",
      "  time_this_iter_s: 0.5058619976043701\n",
      "  time_total_s: 6.933888912200928\n",
      "  timestamp: 1605168166\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 12\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-02-51\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 23\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 12.11029601097107\n",
      "  time_this_iter_s: 0.5698580741882324\n",
      "  time_total_s: 12.11029601097107\n",
      "  timestamp: 1605168171\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 23\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: None<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         12.5665</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         11.5944</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-02-51\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 23\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 12.057617902755737\n",
      "  time_this_iter_s: 0.4631779193878174\n",
      "  time_total_s: 12.057617902755737\n",
      "  timestamp: 1605168171\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 23\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-02-56\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 32\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 17.12488603591919\n",
      "  time_this_iter_s: 0.6398909091949463\n",
      "  time_total_s: 17.12488603591919\n",
      "  timestamp: 1605168176\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 32\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-02-56\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 32\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 17.088084936141968\n",
      "  time_this_iter_s: 0.5243239402770996\n",
      "  time_total_s: 17.088084936141968\n",
      "  timestamp: 1605168176\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 32\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         17.1249</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         17.0881</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-01\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 42\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 22.270148038864136\n",
      "  time_this_iter_s: 0.6429989337921143\n",
      "  time_total_s: 22.270148038864136\n",
      "  timestamp: 1605168181\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 42\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-02\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 42\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 22.373780965805054\n",
      "  time_this_iter_s: 0.7468440532684326\n",
      "  time_total_s: 22.373780965805054\n",
      "  timestamp: 1605168182\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 42\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         22.2701</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         22.3738</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-07\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 51\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 27.764589071273804\n",
      "  time_this_iter_s: 0.5507898330688477\n",
      "  time_total_s: 27.764589071273804\n",
      "  timestamp: 1605168187\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 51\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-07\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 51\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 27.64704203605652\n",
      "  time_this_iter_s: 0.5525281429290771\n",
      "  time_total_s: 27.64704203605652\n",
      "  timestamp: 1605168187\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 51\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         27.7646</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         27.647 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-12\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 60\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 33.13628792762756\n",
      "  time_this_iter_s: 0.6325118541717529\n",
      "  time_total_s: 33.13628792762756\n",
      "  timestamp: 1605168192\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 60\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-12\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 60\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 33.020057916641235\n",
      "  time_this_iter_s: 0.5785958766937256\n",
      "  time_total_s: 33.020057916641235\n",
      "  timestamp: 1605168192\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 60\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         33.1363</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         33.0201</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-17\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 69\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 38.14595413208008\n",
      "  time_this_iter_s: 0.6872799396514893\n",
      "  time_total_s: 38.14595413208008\n",
      "  timestamp: 1605168197\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 69\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         38.693 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         38.0079</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-18\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 70\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 38.53648090362549\n",
      "  time_this_iter_s: 0.5285649299621582\n",
      "  time_total_s: 38.53648090362549\n",
      "  timestamp: 1605168198\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 70\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-22\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 78\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 43.160306215286255\n",
      "  time_this_iter_s: 0.5443289279937744\n",
      "  time_total_s: 43.160306215286255\n",
      "  timestamp: 1605168202\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 78\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         43.9475</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         43.0642</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-23\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 79\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 43.74388575553894\n",
      "  time_this_iter_s: 0.679710865020752\n",
      "  time_total_s: 43.74388575553894\n",
      "  timestamp: 1605168203\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 79\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-27\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 86\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 48.21067690849304\n",
      "  time_this_iter_s: 0.6827027797698975\n",
      "  time_total_s: 48.21067690849304\n",
      "  timestamp: 1605168207\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 86\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         48.7638</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         48.5025</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-28\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 88\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 49.028887033462524\n",
      "  time_this_iter_s: 0.5264120101928711\n",
      "  time_total_s: 49.028887033462524\n",
      "  timestamp: 1605168208\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 88\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-32\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 96\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 53.5206880569458\n",
      "  time_this_iter_s: 0.5069618225097656\n",
      "  time_total_s: 53.5206880569458\n",
      "  timestamp: 1605168212\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 96\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         54.0529</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         53.2828</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-34\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 98\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 54.46523380279541\n",
      "  time_this_iter_s: 0.6697080135345459\n",
      "  time_total_s: 54.46523380279541\n",
      "  timestamp: 1605168214\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 98\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-37\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 104\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 58.610191106796265\n",
      "  time_this_iter_s: 0.6149179935455322\n",
      "  time_total_s: 58.610191106796265\n",
      "  timestamp: 1605168217\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 104\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         59.2577</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         58.42  </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-39\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 106\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 59.784090757369995\n",
      "  time_this_iter_s: 0.7317759990692139\n",
      "  time_total_s: 59.784090757369995\n",
      "  timestamp: 1605168219\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 106\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-43\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 113\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 64.07219290733337\n",
      "  time_this_iter_s: 0.5580117702484131\n",
      "  time_total_s: 64.07219290733337\n",
      "  timestamp: 1605168223\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 113\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: None | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         64.0722</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         63.7563</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-44\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 115\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 64.87160611152649\n",
      "  time_this_iter_s: 0.5609242916107178\n",
      "  time_total_s: 64.87160611152649\n",
      "  timestamp: 1605168224\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 115\n",
      "  trial_id: 6cad7_00001\n",
      "  \n",
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-48\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 123\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 69.53550505638123\n",
      "  time_this_iter_s: 0.5232419967651367\n",
      "  time_total_s: 69.53550505638123\n",
      "  timestamp: 1605168228\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 123\n",
      "  trial_id: 6cad7_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         69.5355</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         68.7026</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-50\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 125\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 70.28128004074097\n",
      "  time_this_iter_s: 0.5224711894989014\n",
      "  time_total_s: 70.28128004074097\n",
      "  timestamp: 1605168230\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 125\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         74.4948</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         74.1083</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-54\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 133\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 75.0378041267395\n",
      "  time_this_iter_s: 0.5430312156677246\n",
      "  time_total_s: 75.0378041267395\n",
      "  timestamp: 1605168234\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 133\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-03-55\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 135\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 75.79805994033813\n",
      "  time_this_iter_s: 0.5456879138946533\n",
      "  time_total_s: 75.79805994033813\n",
      "  timestamp: 1605168235\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 135\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         79.9614</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         78.9792</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-03-59\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 143\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 80.50776696205139\n",
      "  time_this_iter_s: 0.5464119911193848\n",
      "  time_total_s: 80.50776696205139\n",
      "  timestamp: 1605168239\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 143\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-04-00\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 145\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 81.12896394729614\n",
      "  time_this_iter_s: 0.5325238704681396\n",
      "  time_total_s: 81.12896394729614\n",
      "  timestamp: 1605168240\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 145\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         84.9083</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         84.4511</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-04-05\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 153\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 86.15294003486633\n",
      "  time_this_iter_s: 0.6509630680084229\n",
      "  time_total_s: 86.15294003486633\n",
      "  timestamp: 1605168245\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 153\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-04-06\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 154\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 86.28704476356506\n",
      "  time_this_iter_s: 0.6073489189147949\n",
      "  time_total_s: 86.28704476356506\n",
      "  timestamp: 1605168246\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 154\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         89.9412</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         89.4934</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-04-10\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 163\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 91.35974597930908\n",
      "  time_this_iter_s: 0.45238780975341797\n",
      "  time_total_s: 91.35974597930908\n",
      "  timestamp: 1605168250\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 163\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-04-11\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 164\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 91.32333493232727\n",
      "  time_this_iter_s: 0.4682149887084961\n",
      "  time_total_s: 91.32333493232727\n",
      "  timestamp: 1605168251\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 164\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         95.1619</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         94.2307</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-04-15\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 174\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 96.62815308570862\n",
      "  time_this_iter_s: 0.5629069805145264\n",
      "  time_total_s: 96.62815308570862\n",
      "  timestamp: 1605168255\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 174\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-04-16\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 175\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 96.58736205101013\n",
      "  time_this_iter_s: 0.4444301128387451\n",
      "  time_total_s: 96.58736205101013\n",
      "  timestamp: 1605168256\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 175\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">        100.243 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         99.2629</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-04-21\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 186\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 101.9655110836029\n",
      "  time_this_iter_s: 0.4356238842010498\n",
      "  time_total_s: 101.9655110836029\n",
      "  timestamp: 1605168261\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 186\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-04-21\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 187\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 101.99704813957214\n",
      "  time_this_iter_s: 0.5632832050323486\n",
      "  time_total_s: 101.99704813957214\n",
      "  timestamp: 1605168261\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 187\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 4/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>RUNNING </td><td>192.168.0.173:92583</td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">          104.75</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>RUNNING </td><td>192.168.0.173:92581</td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">          104.8 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>PENDING </td><td>                   </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00000:\n",
      "  date: 2020-11-12_12-04-26\n",
      "  done: false\n",
      "  experiment_id: d205b20c88a943e6b006f097b7c8dc2a\n",
      "  experiment_tag: 0_hidden=6,lr=0.047251,momentum=0.64299\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 196\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92583\n",
      "  time_since_restore: 107.17751216888428\n",
      "  time_this_iter_s: 0.41877293586730957\n",
      "  time_total_s: 107.17751216888428\n",
      "  timestamp: 1605168266\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 196\n",
      "  trial_id: 6cad7_00000\n",
      "  \n",
      "Result for train_wildfire_6cad7_00001:\n",
      "  date: 2020-11-12_12-04-26\n",
      "  done: false\n",
      "  experiment_id: 949086fc57204aa5ad7f0079905beaed\n",
      "  experiment_tag: 1_hidden=5,lr=0.030943,momentum=0.37669\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 197\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92581\n",
      "  time_since_restore: 107.07506394386292\n",
      "  time_this_iter_s: 0.44118309020996094\n",
      "  time_total_s: 107.07506394386292\n",
      "  timestamp: 1605168266\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 197\n",
      "  trial_id: 6cad7_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=92582)\u001b[0m 2020-11-12 12:04:32.028836: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=92582)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(pid=92582)\u001b[0m 2020-11-12 12:04:32.045100: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fe055e0ccb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[2m\u001b[36m(pid=92582)\u001b[0m 2020-11-12 12:04:32.045148: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-04-32\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 1\n",
      "  mean_accuracy: 0.14835482835769653\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 0.8983039855957031\n",
      "  time_this_iter_s: 0.8983039855957031\n",
      "  time_total_s: 0.8983039855957031\n",
      "  timestamp: 1605168272\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148355</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.898304</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">      108.967   </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">      108.416   </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-04-38\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 18\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 6.191724061965942\n",
      "  time_this_iter_s: 0.30022716522216797\n",
      "  time_total_s: 6.191724061965942\n",
      "  timestamp: 1605168278\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 18\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         6.19172</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">       108.967  </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">       108.416  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-04-43\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 33\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 11.287472009658813\n",
      "  time_this_iter_s: 0.35499095916748047\n",
      "  time_total_s: 11.287472009658813\n",
      "  timestamp: 1605168283\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 33\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         11.2875</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-04-48\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 48\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 16.58076310157776\n",
      "  time_this_iter_s: 0.3234119415283203\n",
      "  time_total_s: 16.58076310157776\n",
      "  timestamp: 1605168288\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 48\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         16.5808</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-04-53\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 63\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 21.744102001190186\n",
      "  time_this_iter_s: 0.34343791007995605\n",
      "  time_total_s: 21.744102001190186\n",
      "  timestamp: 1605168293\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 63\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         21.7441</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-04-59\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 78\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 26.98603892326355\n",
      "  time_this_iter_s: 0.3387718200683594\n",
      "  time_total_s: 26.98603892326355\n",
      "  timestamp: 1605168299\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 78\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          26.986</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         108.967</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         108.416</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-04\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 94\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 32.21502614021301\n",
      "  time_this_iter_s: 0.2904491424560547\n",
      "  time_total_s: 32.21502614021301\n",
      "  timestamp: 1605168304\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 94\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">          32.215</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         108.967</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         108.416</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-09\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 110\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 37.400481939315796\n",
      "  time_this_iter_s: 0.3332788944244385\n",
      "  time_total_s: 37.400481939315796\n",
      "  timestamp: 1605168309\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 110\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         37.4005</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-14\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 125\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 42.50770616531372\n",
      "  time_this_iter_s: 0.318026065826416\n",
      "  time_total_s: 42.50770616531372\n",
      "  timestamp: 1605168314\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 125\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         42.5077</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-19\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 140\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 47.69974708557129\n",
      "  time_this_iter_s: 0.36446499824523926\n",
      "  time_total_s: 47.69974708557129\n",
      "  timestamp: 1605168319\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 140\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         47.6997</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-24\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 155\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 52.89549517631531\n",
      "  time_this_iter_s: 0.32807111740112305\n",
      "  time_total_s: 52.89549517631531\n",
      "  timestamp: 1605168324\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 155\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         52.8955</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-30\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 170\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 58.093072175979614\n",
      "  time_this_iter_s: 0.32991623878479004\n",
      "  time_total_s: 58.093072175979614\n",
      "  timestamp: 1605168330\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 170\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         58.0931</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for train_wildfire_6cad7_00002:\n",
      "  date: 2020-11-12_12-05-35\n",
      "  done: false\n",
      "  experiment_id: 92e2e6ce801c41f7b7603a1f56306056\n",
      "  experiment_tag: 2_hidden=1,lr=0.027058,momentum=0.89463\n",
      "  hostname: salihas-MBP\n",
      "  iterations_since_restore: 186\n",
      "  mean_accuracy: 0.1480856090784073\n",
      "  node_ip: 192.168.0.173\n",
      "  pid: 92582\n",
      "  time_since_restore: 63.36606192588806\n",
      "  time_this_iter_s: 0.3408689498901367\n",
      "  time_total_s: 63.36606192588806\n",
      "  timestamp: 1605168335\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 186\n",
      "  trial_id: 6cad7_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 2/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>RUNNING   </td><td>192.168.0.173:92582</td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         63.3661</td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using AsyncHyperBand: num_stopped=0\n",
       "Bracket: Iter 120.000: 0.1480856090784073 | Iter 30.000: 0.1480856090784073<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/5.22 GiB heap, 0.0/1.81 GiB objects<br>Result logdir: /Users/salihamehboob/ray_results/exp<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  hidden</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">     acc</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_wildfire_6cad7_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       6</td><td style=\"text-align: right;\">0.0472511</td><td style=\"text-align: right;\">  0.642994</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.967 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       5</td><td style=\"text-align: right;\">0.030943 </td><td style=\"text-align: right;\">  0.376692</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">        108.416 </td></tr>\n",
       "<tr><td>train_wildfire_6cad7_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">       1</td><td style=\"text-align: right;\">0.0270583</td><td style=\"text-align: right;\">  0.894625</td><td style=\"text-align: right;\">0.148086</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         68.1216</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-12 12:05:40,157\tINFO tune.py:439 -- Total run time: 184.81 seconds (184.58 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import ray\n",
    "    from ray import tune\n",
    "    from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "    ray.shutdown()\n",
    "    mnist.load_data()  # we do this on the driver because it's not threadsafe\n",
    "\n",
    "    ray.init(num_cpus=4 if args.smoke_test else None)\n",
    "    sched = AsyncHyperBandScheduler(\n",
    "        time_attr=\"training_iteration\",\n",
    "        metric=\"mean_accuracy\",\n",
    "        mode=\"max\",\n",
    "        max_t=400,\n",
    "        grace_period=30)\n",
    "\n",
    "    tune.run(\n",
    "        train_wildfire,\n",
    "        name=\"exp\",\n",
    "        scheduler=sched,\n",
    "        stop={\n",
    "            \"mean_accuracy\": 0.70,\n",
    "            \"training_iteration\": 5 if args.smoke_test else 300\n",
    "        },\n",
    "        num_samples=3,\n",
    "        resources_per_trial={\n",
    "            \"cpu\": 2,\n",
    "            \"gpu\": 0\n",
    "        },\n",
    "        config={\n",
    "            \"threads\": 2,\n",
    "            \"lr\": tune.sample_from(lambda spec: np.random.uniform(0.01, 0.1)),\n",
    "            \"momentum\": tune.sample_from(\n",
    "                lambda spec: np.random.uniform(0.1, 0.9)),\n",
    "            \"hidden\": tune.sample_from(\n",
    "                lambda spec: np.random.randint(1, 10)),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import tensorflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rarfile import RarFile\n",
    "from urllib.request import urlretrieve\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set(rc={'figure.figsize': (20, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        # Specify model\n",
    "        model = keras.Sequential()\n",
    "\n",
    "        # Range of models to build\n",
    "        for i in range(hp.Int('num_layers', 2, 10)):\n",
    "\n",
    "            model.add(keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                                min_value=1,\n",
    "                                                max_value=10, \n",
    "                                                step=1),\n",
    "                                   activation='relu'))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(keras.layers.Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        # Compile the constructed model and return it\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Choice('learning_rate',\n",
    "                          values=[1e-2, 1e-3, 1e-4,1e-1,1e0])),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "\n",
    "hypermodel = MyHyperModel(num_classes=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6868/9403 [====================>.........] - ETA: 0s - loss: 1.0986 - accuracy: 0.21 - ETA: 13s - loss: 1.0981 - accuracy: 0.428 - ETA: 15s - loss: 1.0977 - accuracy: 0.446 - ETA: 15s - loss: 1.0972 - accuracy: 0.447 - ETA: 13s - loss: 1.0966 - accuracy: 0.444 - ETA: 13s - loss: 1.0962 - accuracy: 0.442 - ETA: 12s - loss: 1.0955 - accuracy: 0.441 - ETA: 12s - loss: 1.0949 - accuracy: 0.440 - ETA: 13s - loss: 1.0948 - accuracy: 0.440 - ETA: 16s - loss: 1.0948 - accuracy: 0.440 - ETA: 15s - loss: 1.0942 - accuracy: 0.441 - ETA: 15s - loss: 1.0938 - accuracy: 0.442 - ETA: 14s - loss: 1.0930 - accuracy: 0.441 - ETA: 14s - loss: 1.0924 - accuracy: 0.443 - ETA: 13s - loss: 1.0917 - accuracy: 0.444 - ETA: 13s - loss: 1.0911 - accuracy: 0.441 - ETA: 12s - loss: 1.0903 - accuracy: 0.441 - ETA: 12s - loss: 1.0898 - accuracy: 0.440 - ETA: 12s - loss: 1.0893 - accuracy: 0.440 - ETA: 12s - loss: 1.0886 - accuracy: 0.441 - ETA: 11s - loss: 1.0879 - accuracy: 0.443 - ETA: 12s - loss: 1.0875 - accuracy: 0.444 - ETA: 12s - loss: 1.0873 - accuracy: 0.443 - ETA: 12s - loss: 1.0872 - accuracy: 0.443 - ETA: 12s - loss: 1.0869 - accuracy: 0.443 - ETA: 12s - loss: 1.0863 - accuracy: 0.444 - ETA: 12s - loss: 1.0861 - accuracy: 0.443 - ETA: 12s - loss: 1.0855 - accuracy: 0.444 - ETA: 12s - loss: 1.0848 - accuracy: 0.446 - ETA: 12s - loss: 1.0846 - accuracy: 0.446 - ETA: 12s - loss: 1.0840 - accuracy: 0.446 - ETA: 12s - loss: 1.0836 - accuracy: 0.445 - ETA: 12s - loss: 1.0831 - accuracy: 0.444 - ETA: 12s - loss: 1.0827 - accuracy: 0.444 - ETA: 12s - loss: 1.0821 - accuracy: 0.443 - ETA: 12s - loss: 1.0817 - accuracy: 0.444 - ETA: 11s - loss: 1.0810 - accuracy: 0.443 - ETA: 11s - loss: 1.0805 - accuracy: 0.443 - ETA: 11s - loss: 1.0803 - accuracy: 0.443 - ETA: 11s - loss: 1.0798 - accuracy: 0.443 - ETA: 11s - loss: 1.0792 - accuracy: 0.443 - ETA: 11s - loss: 1.0790 - accuracy: 0.443 - ETA: 11s - loss: 1.0784 - accuracy: 0.443 - ETA: 11s - loss: 1.0779 - accuracy: 0.443 - ETA: 11s - loss: 1.0773 - accuracy: 0.443 - ETA: 11s - loss: 1.0769 - accuracy: 0.443 - ETA: 10s - loss: 1.0764 - accuracy: 0.442 - ETA: 10s - loss: 1.0759 - accuracy: 0.443 - ETA: 10s - loss: 1.0755 - accuracy: 0.443 - ETA: 10s - loss: 1.0751 - accuracy: 0.442 - ETA: 10s - loss: 1.0747 - accuracy: 0.442 - ETA: 10s - loss: 1.0744 - accuracy: 0.442 - ETA: 10s - loss: 1.0739 - accuracy: 0.442 - ETA: 10s - loss: 1.0735 - accuracy: 0.442 - ETA: 10s - loss: 1.0730 - accuracy: 0.441 - ETA: 10s - loss: 1.0724 - accuracy: 0.442 - ETA: 9s - loss: 1.0719 - accuracy: 0.442 - ETA: 9s - loss: 1.0715 - accuracy: 0.44 - ETA: 9s - loss: 1.0713 - accuracy: 0.44 - ETA: 9s - loss: 1.0709 - accuracy: 0.44 - ETA: 9s - loss: 1.0704 - accuracy: 0.44 - ETA: 9s - loss: 1.0702 - accuracy: 0.44 - ETA: 9s - loss: 1.0697 - accuracy: 0.44 - ETA: 9s - loss: 1.0694 - accuracy: 0.44 - ETA: 9s - loss: 1.0690 - accuracy: 0.44 - ETA: 9s - loss: 1.0688 - accuracy: 0.44 - ETA: 9s - loss: 1.0684 - accuracy: 0.44 - ETA: 9s - loss: 1.0681 - accuracy: 0.44 - ETA: 9s - loss: 1.0679 - accuracy: 0.44 - ETA: 9s - loss: 1.0676 - accuracy: 0.44 - ETA: 9s - loss: 1.0672 - accuracy: 0.44 - ETA: 9s - loss: 1.0670 - accuracy: 0.44 - ETA: 9s - loss: 1.0667 - accuracy: 0.44 - ETA: 9s - loss: 1.0664 - accuracy: 0.44 - ETA: 9s - loss: 1.0661 - accuracy: 0.44 - ETA: 9s - loss: 1.0658 - accuracy: 0.44 - ETA: 9s - loss: 1.0654 - accuracy: 0.44 - ETA: 9s - loss: 1.0651 - accuracy: 0.44 - ETA: 9s - loss: 1.0648 - accuracy: 0.44 - ETA: 9s - loss: 1.0645 - accuracy: 0.44 - ETA: 9s - loss: 1.0641 - accuracy: 0.44 - ETA: 8s - loss: 1.0638 - accuracy: 0.44 - ETA: 8s - loss: 1.0636 - accuracy: 0.44 - ETA: 8s - loss: 1.0633 - accuracy: 0.44 - ETA: 8s - loss: 1.0630 - accuracy: 0.44 - ETA: 8s - loss: 1.0627 - accuracy: 0.44 - ETA: 8s - loss: 1.0624 - accuracy: 0.44 - ETA: 8s - loss: 1.0622 - accuracy: 0.44 - ETA: 8s - loss: 1.0617 - accuracy: 0.44 - ETA: 8s - loss: 1.0615 - accuracy: 0.44 - ETA: 8s - loss: 1.0615 - accuracy: 0.44 - ETA: 8s - loss: 1.0611 - accuracy: 0.44 - ETA: 8s - loss: 1.0609 - accuracy: 0.44 - ETA: 8s - loss: 1.0606 - accuracy: 0.44 - ETA: 8s - loss: 1.0601 - accuracy: 0.44 - ETA: 8s - loss: 1.0598 - accuracy: 0.44 - ETA: 8s - loss: 1.0596 - accuracy: 0.44 - ETA: 8s - loss: 1.0593 - accuracy: 0.44 - ETA: 8s - loss: 1.0591 - accuracy: 0.44 - ETA: 7s - loss: 1.0587 - accuracy: 0.44 - ETA: 7s - loss: 1.0585 - accuracy: 0.44 - ETA: 7s - loss: 1.0582 - accuracy: 0.44 - ETA: 7s - loss: 1.0578 - accuracy: 0.44 - ETA: 7s - loss: 1.0574 - accuracy: 0.44 - ETA: 7s - loss: 1.0571 - accuracy: 0.44 - ETA: 7s - loss: 1.0567 - accuracy: 0.44 - ETA: 7s - loss: 1.0565 - accuracy: 0.44 - ETA: 7s - loss: 1.0561 - accuracy: 0.44 - ETA: 7s - loss: 1.0560 - accuracy: 0.44 - ETA: 7s - loss: 1.0558 - accuracy: 0.44 - ETA: 7s - loss: 1.0555 - accuracy: 0.44 - ETA: 7s - loss: 1.0553 - accuracy: 0.44 - ETA: 7s - loss: 1.0551 - accuracy: 0.44 - ETA: 7s - loss: 1.0548 - accuracy: 0.44 - ETA: 6s - loss: 1.0545 - accuracy: 0.44 - ETA: 6s - loss: 1.0541 - accuracy: 0.44 - ETA: 6s - loss: 1.0538 - accuracy: 0.44 - ETA: 6s - loss: 1.0537 - accuracy: 0.44 - ETA: 6s - loss: 1.0537 - accuracy: 0.44 - ETA: 6s - loss: 1.0535 - accuracy: 0.44 - ETA: 6s - loss: 1.0534 - accuracy: 0.44 - ETA: 6s - loss: 1.0532 - accuracy: 0.44 - ETA: 6s - loss: 1.0530 - accuracy: 0.44 - ETA: 6s - loss: 1.0528 - accuracy: 0.44 - ETA: 6s - loss: 1.0527 - accuracy: 0.44 - ETA: 6s - loss: 1.0526 - accuracy: 0.44 - ETA: 6s - loss: 1.0523 - accuracy: 0.44 - ETA: 6s - loss: 1.0521 - accuracy: 0.44 - ETA: 6s - loss: 1.0519 - accuracy: 0.44 - ETA: 6s - loss: 1.0517 - accuracy: 0.44 - ETA: 6s - loss: 1.0515 - accuracy: 0.44 - ETA: 6s - loss: 1.0513 - accuracy: 0.44 - ETA: 6s - loss: 1.0511 - accuracy: 0.44 - ETA: 6s - loss: 1.0509 - accuracy: 0.44 - ETA: 6s - loss: 1.0507 - accuracy: 0.44 - ETA: 6s - loss: 1.0505 - accuracy: 0.44 - ETA: 6s - loss: 1.0502 - accuracy: 0.44 - ETA: 6s - loss: 1.0501 - accuracy: 0.44 - ETA: 6s - loss: 1.0499 - accuracy: 0.44 - ETA: 6s - loss: 1.0498 - accuracy: 0.44 - ETA: 5s - loss: 1.0496 - accuracy: 0.44 - ETA: 5s - loss: 1.0495 - accuracy: 0.44 - ETA: 5s - loss: 1.0493 - accuracy: 0.44 - ETA: 5s - loss: 1.0492 - accuracy: 0.44 - ETA: 5s - loss: 1.0491 - accuracy: 0.44 - ETA: 5s - loss: 1.0490 - accuracy: 0.44 - ETA: 5s - loss: 1.0489 - accuracy: 0.44 - ETA: 5s - loss: 1.0486 - accuracy: 0.44 - ETA: 5s - loss: 1.0484 - accuracy: 0.44 - ETA: 5s - loss: 1.0483 - accuracy: 0.44 - ETA: 5s - loss: 1.0480 - accuracy: 0.44 - ETA: 5s - loss: 1.0478 - accuracy: 0.44 - ETA: 5s - loss: 1.0476 - accuracy: 0.44 - ETA: 5s - loss: 1.0474 - accuracy: 0.44 - ETA: 5s - loss: 1.0471 - accuracy: 0.44 - ETA: 5s - loss: 1.0469 - accuracy: 0.44 - ETA: 5s - loss: 1.0467 - accuracy: 0.44 - ETA: 5s - loss: 1.0464 - accuracy: 0.44 - ETA: 5s - loss: 1.0462 - accuracy: 0.44 - ETA: 5s - loss: 1.0460 - accuracy: 0.44 - ETA: 5s - loss: 1.0458 - accuracy: 0.44 - ETA: 4s - loss: 1.0456 - accuracy: 0.44 - ETA: 4s - loss: 1.0454 - accuracy: 0.44 - ETA: 4s - loss: 1.0453 - accuracy: 0.44 - ETA: 4s - loss: 1.0451 - accuracy: 0.44 - ETA: 4s - loss: 1.0449 - accuracy: 0.44 - ETA: 4s - loss: 1.0447 - accuracy: 0.44 - ETA: 4s - loss: 1.0445 - accuracy: 0.44 - ETA: 4s - loss: 1.0444 - accuracy: 0.44 - ETA: 4s - loss: 1.0443 - accuracy: 0.44 - ETA: 4s - loss: 1.0443 - accuracy: 0.44 - ETA: 4s - loss: 1.0441 - accuracy: 0.44 - ETA: 4s - loss: 1.0440 - accuracy: 0.44 - ETA: 4s - loss: 1.0439 - accuracy: 0.44 - ETA: 4s - loss: 1.0438 - accuracy: 0.44 - ETA: 4s - loss: 1.0437 - accuracy: 0.44 - ETA: 4s - loss: 1.0434 - accuracy: 0.44 - ETA: 4s - loss: 1.0432 - accuracy: 0.44 - ETA: 4s - loss: 1.0431 - accuracy: 0.44 - ETA: 4s - loss: 1.0430 - accuracy: 0.44 - ETA: 4s - loss: 1.0428 - accuracy: 0.44 - ETA: 4s - loss: 1.0426 - accuracy: 0.44 - ETA: 4s - loss: 1.0425 - accuracy: 0.44 - ETA: 4s - loss: 1.0423 - accuracy: 0.44 - ETA: 3s - loss: 1.0421 - accuracy: 0.44 - ETA: 3s - loss: 1.0420 - accuracy: 0.44 - ETA: 3s - loss: 1.0418 - accuracy: 0.44 - ETA: 3s - loss: 1.0416 - accuracy: 0.44 - ETA: 3s - loss: 1.0414 - accuracy: 0.44 - ETA: 3s - loss: 1.0413 - accuracy: 0.44 - ETA: 3s - loss: 1.0411 - accuracy: 0.44 - ETA: 3s - loss: 1.0410 - accuracy: 0.4414\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 3s - loss: 1.0410 - accuracy: 0.44 - ETA: 3s - loss: 1.0409 - accuracy: 0.44 - ETA: 3s - loss: 1.0408 - accuracy: 0.44 - ETA: 3s - loss: 1.0407 - accuracy: 0.44 - ETA: 3s - loss: 1.0406 - accuracy: 0.44 - ETA: 3s - loss: 1.0403 - accuracy: 0.44 - ETA: 3s - loss: 1.0401 - accuracy: 0.44 - ETA: 3s - loss: 1.0400 - accuracy: 0.44 - ETA: 3s - loss: 1.0400 - accuracy: 0.44 - ETA: 3s - loss: 1.0399 - accuracy: 0.44 - ETA: 3s - loss: 1.0398 - accuracy: 0.44 - ETA: 2s - loss: 1.0397 - accuracy: 0.44 - ETA: 2s - loss: 1.0396 - accuracy: 0.44 - ETA: 2s - loss: 1.0394 - accuracy: 0.44 - ETA: 2s - loss: 1.0392 - accuracy: 0.44 - ETA: 2s - loss: 1.0391 - accuracy: 0.44 - ETA: 2s - loss: 1.0391 - accuracy: 0.44 - ETA: 2s - loss: 1.0389 - accuracy: 0.44 - ETA: 2s - loss: 1.0387 - accuracy: 0.44 - ETA: 2s - loss: 1.0386 - accuracy: 0.44 - ETA: 2s - loss: 1.0385 - accuracy: 0.44 - ETA: 2s - loss: 1.0383 - accuracy: 0.44 - ETA: 2s - loss: 1.0382 - accuracy: 0.44 - ETA: 2s - loss: 1.0381 - accuracy: 0.44 - ETA: 2s - loss: 1.0380 - accuracy: 0.44 - ETA: 2s - loss: 1.0379 - accuracy: 0.44 - ETA: 2s - loss: 1.0376 - accuracy: 0.44 - ETA: 2s - loss: 1.0375 - accuracy: 0.44 - ETA: 2s - loss: 1.0375 - accuracy: 0.44 - ETA: 2s - loss: 1.0374 - accuracy: 0.44 - ETA: 1s - loss: 1.0373 - accuracy: 0.44 - ETA: 1s - loss: 1.0371 - accuracy: 0.44 - ETA: 1s - loss: 1.0370 - accuracy: 0.44 - ETA: 1s - loss: 1.0369 - accuracy: 0.44 - ETA: 1s - loss: 1.0368 - accuracy: 0.44 - ETA: 1s - loss: 1.0366 - accuracy: 0.44 - ETA: 1s - loss: 1.0365 - accuracy: 0.44 - ETA: 1s - loss: 1.0364 - accuracy: 0.44 - ETA: 1s - loss: 1.0364 - accuracy: 0.44 - ETA: 1s - loss: 1.0363 - accuracy: 0.44 - ETA: 1s - loss: 1.0362 - accuracy: 0.44 - ETA: 1s - loss: 1.0362 - accuracy: 0.44 - ETA: 1s - loss: 1.0362 - accuracy: 0.44 - ETA: 1s - loss: 1.0361 - accuracy: 0.44 - ETA: 1s - loss: 1.0361 - accuracy: 0.44 - ETA: 1s - loss: 1.0359 - accuracy: 0.44 - ETA: 1s - loss: 1.0359 - accuracy: 0.44 - ETA: 1s - loss: 1.0359 - accuracy: 0.44 - ETA: 1s - loss: 1.0358 - accuracy: 0.44 - ETA: 1s - loss: 1.0357 - accuracy: 0.44 - ETA: 1s - loss: 1.0357 - accuracy: 0.44 - ETA: 1s - loss: 1.0356 - accuracy: 0.44 - ETA: 1s - loss: 1.0355 - accuracy: 0.44 - ETA: 1s - loss: 1.0354 - accuracy: 0.44 - ETA: 0s - loss: 1.0353 - accuracy: 0.44 - ETA: 0s - loss: 1.0352 - accuracy: 0.44 - ETA: 0s - loss: 1.0351 - accuracy: 0.44 - ETA: 0s - loss: 1.0349 - accuracy: 0.44 - ETA: 0s - loss: 1.0348 - accuracy: 0.44 - ETA: 0s - loss: 1.0348 - accuracy: 0.44 - ETA: 0s - loss: 1.0346 - accuracy: 0.44 - ETA: 0s - loss: 1.0345 - accuracy: 0.44 - ETA: 0s - loss: 1.0344 - accuracy: 0.44 - ETA: 0s - loss: 1.0343 - accuracy: 0.44 - ETA: 0s - loss: 1.0342 - accuracy: 0.44 - ETA: 0s - loss: 1.0340 - accuracy: 0.44 - ETA: 0s - loss: 1.0339 - accuracy: 0.44 - ETA: 0s - loss: 1.0338 - accuracy: 0.44 - ETA: 0s - loss: 1.0337 - accuracy: 0.44 - ETA: 0s - loss: 1.0336 - accuracy: 0.44 - ETA: 0s - loss: 1.0335 - accuracy: 0.44 - ETA: 0s - loss: 1.0334 - accuracy: 0.44 - ETA: 0s - loss: 1.0334 - accuracy: 0.44 - 15s 2ms/step - loss: 1.0334 - accuracy: 0.4414 - val_loss: 1.0107 - val_accuracy: 0.4414\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9105/9403 [============================>.] - ETA: 0s - loss: 0.9359 - accuracy: 0.43 - ETA: 7s - loss: 0.9950 - accuracy: 0.45 - ETA: 9s - loss: 1.0026 - accuracy: 0.43 - ETA: 8s - loss: 1.0049 - accuracy: 0.44 - ETA: 8s - loss: 1.0062 - accuracy: 0.44 - ETA: 8s - loss: 1.0070 - accuracy: 0.44 - ETA: 8s - loss: 1.0072 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.43 - ETA: 8s - loss: 1.0087 - accuracy: 0.44 - ETA: 8s - loss: 1.0077 - accuracy: 0.44 - ETA: 8s - loss: 1.0079 - accuracy: 0.44 - ETA: 8s - loss: 1.0094 - accuracy: 0.43 - ETA: 8s - loss: 1.0092 - accuracy: 0.43 - ETA: 8s - loss: 1.0102 - accuracy: 0.43 - ETA: 8s - loss: 1.0099 - accuracy: 0.43 - ETA: 9s - loss: 1.0100 - accuracy: 0.43 - ETA: 9s - loss: 1.0105 - accuracy: 0.43 - ETA: 10s - loss: 1.0105 - accuracy: 0.438 - ETA: 9s - loss: 1.0119 - accuracy: 0.438 - ETA: 9s - loss: 1.0112 - accuracy: 0.43 - ETA: 9s - loss: 1.0120 - accuracy: 0.43 - ETA: 9s - loss: 1.0113 - accuracy: 0.43 - ETA: 9s - loss: 1.0113 - accuracy: 0.43 - ETA: 9s - loss: 1.0111 - accuracy: 0.44 - ETA: 9s - loss: 1.0108 - accuracy: 0.44 - ETA: 9s - loss: 1.0109 - accuracy: 0.44 - ETA: 9s - loss: 1.0112 - accuracy: 0.44 - ETA: 9s - loss: 1.0104 - accuracy: 0.44 - ETA: 9s - loss: 1.0095 - accuracy: 0.44 - ETA: 9s - loss: 1.0096 - accuracy: 0.44 - ETA: 9s - loss: 1.0092 - accuracy: 0.44 - ETA: 9s - loss: 1.0090 - accuracy: 0.44 - ETA: 9s - loss: 1.0091 - accuracy: 0.44 - ETA: 8s - loss: 1.0090 - accuracy: 0.44 - ETA: 8s - loss: 1.0091 - accuracy: 0.44 - ETA: 8s - loss: 1.0091 - accuracy: 0.44 - ETA: 9s - loss: 1.0095 - accuracy: 0.44 - ETA: 9s - loss: 1.0098 - accuracy: 0.44 - ETA: 9s - loss: 1.0099 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.44 - ETA: 8s - loss: 1.0105 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0099 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0104 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.44 - ETA: 8s - loss: 1.0103 - accuracy: 0.43 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 6s - loss: 1.0100 - accuracy: 0.44 - ETA: 6s - loss: 1.0098 - accuracy: 0.44 - ETA: 6s - loss: 1.0097 - accuracy: 0.44 - ETA: 6s - loss: 1.0097 - accuracy: 0.44 - ETA: 6s - loss: 1.0097 - accuracy: 0.43 - ETA: 6s - loss: 1.0098 - accuracy: 0.43 - ETA: 6s - loss: 1.0100 - accuracy: 0.43 - ETA: 6s - loss: 1.0099 - accuracy: 0.43 - ETA: 6s - loss: 1.0098 - accuracy: 0.43 - ETA: 6s - loss: 1.0098 - accuracy: 0.44 - ETA: 6s - loss: 1.0097 - accuracy: 0.43 - ETA: 6s - loss: 1.0098 - accuracy: 0.44 - ETA: 6s - loss: 1.0096 - accuracy: 0.44 - ETA: 5s - loss: 1.0093 - accuracy: 0.44 - ETA: 5s - loss: 1.0094 - accuracy: 0.44 - ETA: 5s - loss: 1.0095 - accuracy: 0.44 - ETA: 5s - loss: 1.0092 - accuracy: 0.44 - ETA: 5s - loss: 1.0091 - accuracy: 0.44 - ETA: 5s - loss: 1.0092 - accuracy: 0.44 - ETA: 5s - loss: 1.0093 - accuracy: 0.44 - ETA: 5s - loss: 1.0093 - accuracy: 0.44 - ETA: 5s - loss: 1.0094 - accuracy: 0.44 - ETA: 5s - loss: 1.0094 - accuracy: 0.44 - ETA: 5s - loss: 1.0095 - accuracy: 0.44 - ETA: 5s - loss: 1.0094 - accuracy: 0.44 - ETA: 5s - loss: 1.0096 - accuracy: 0.44 - ETA: 5s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0099 - accuracy: 0.44 - ETA: 3s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0098 - accuracy: 0.44 - ETA: 0s - loss: 1.0098 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0098 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.4411"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - 12s 1ms/step - loss: 1.0096 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n",
      "Epoch 3/10\n",
      "9403/9403 [==============================] - ETA: 0s - loss: 0.9453 - accuracy: 0.56 - ETA: 7s - loss: 1.0122 - accuracy: 0.42 - ETA: 7s - loss: 1.0078 - accuracy: 0.43 - ETA: 7s - loss: 1.0049 - accuracy: 0.44 - ETA: 7s - loss: 1.0047 - accuracy: 0.44 - ETA: 7s - loss: 1.0066 - accuracy: 0.44 - ETA: 7s - loss: 1.0046 - accuracy: 0.44 - ETA: 7s - loss: 1.0043 - accuracy: 0.44 - ETA: 8s - loss: 1.0046 - accuracy: 0.44 - ETA: 8s - loss: 1.0050 - accuracy: 0.44 - ETA: 8s - loss: 1.0049 - accuracy: 0.44 - ETA: 8s - loss: 1.0058 - accuracy: 0.44 - ETA: 8s - loss: 1.0063 - accuracy: 0.44 - ETA: 8s - loss: 1.0069 - accuracy: 0.44 - ETA: 8s - loss: 1.0073 - accuracy: 0.44 - ETA: 8s - loss: 1.0091 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.43 - ETA: 7s - loss: 1.0106 - accuracy: 0.43 - ETA: 7s - loss: 1.0101 - accuracy: 0.43 - ETA: 7s - loss: 1.0102 - accuracy: 0.43 - ETA: 7s - loss: 1.0110 - accuracy: 0.43 - ETA: 7s - loss: 1.0107 - accuracy: 0.43 - ETA: 7s - loss: 1.0103 - accuracy: 0.43 - ETA: 7s - loss: 1.0108 - accuracy: 0.43 - ETA: 7s - loss: 1.0107 - accuracy: 0.43 - ETA: 7s - loss: 1.0104 - accuracy: 0.43 - ETA: 7s - loss: 1.0105 - accuracy: 0.43 - ETA: 7s - loss: 1.0106 - accuracy: 0.43 - ETA: 7s - loss: 1.0106 - accuracy: 0.43 - ETA: 7s - loss: 1.0109 - accuracy: 0.43 - ETA: 7s - loss: 1.0107 - accuracy: 0.43 - ETA: 7s - loss: 1.0111 - accuracy: 0.43 - ETA: 7s - loss: 1.0110 - accuracy: 0.43 - ETA: 7s - loss: 1.0112 - accuracy: 0.43 - ETA: 7s - loss: 1.0113 - accuracy: 0.43 - ETA: 6s - loss: 1.0115 - accuracy: 0.43 - ETA: 6s - loss: 1.0112 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.43 - ETA: 6s - loss: 1.0104 - accuracy: 0.43 - ETA: 6s - loss: 1.0102 - accuracy: 0.43 - ETA: 6s - loss: 1.0100 - accuracy: 0.43 - ETA: 6s - loss: 1.0100 - accuracy: 0.43 - ETA: 6s - loss: 1.0103 - accuracy: 0.43 - ETA: 6s - loss: 1.0102 - accuracy: 0.43 - ETA: 6s - loss: 1.0105 - accuracy: 0.43 - ETA: 6s - loss: 1.0109 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.43 - ETA: 6s - loss: 1.0110 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.44 - ETA: 6s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0107 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.43 - ETA: 5s - loss: 1.0106 - accuracy: 0.43 - ETA: 5s - loss: 1.0105 - accuracy: 0.43 - ETA: 5s - loss: 1.0104 - accuracy: 0.43 - ETA: 4s - loss: 1.0104 - accuracy: 0.43 - ETA: 4s - loss: 1.0102 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0102 - accuracy: 0.43 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0102 - accuracy: 0.44 - ETA: 4s - loss: 1.0102 - accuracy: 0.44 - ETA: 4s - loss: 1.0102 - accuracy: 0.44 - ETA: 4s - loss: 1.0102 - accuracy: 0.43 - ETA: 4s - loss: 1.0100 - accuracy: 0.43 - ETA: 4s - loss: 1.0100 - accuracy: 0.43 - ETA: 4s - loss: 1.0102 - accuracy: 0.43 - ETA: 4s - loss: 1.0101 - accuracy: 0.43 - ETA: 4s - loss: 1.0102 - accuracy: 0.43 - ETA: 4s - loss: 1.0104 - accuracy: 0.43 - ETA: 4s - loss: 1.0105 - accuracy: 0.43 - ETA: 4s - loss: 1.0103 - accuracy: 0.43 - ETA: 4s - loss: 1.0102 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 3s - loss: 1.0100 - accuracy: 0.43 - ETA: 3s - loss: 1.0101 - accuracy: 0.43 - ETA: 3s - loss: 1.0101 - accuracy: 0.43 - ETA: 3s - loss: 1.0102 - accuracy: 0.43 - ETA: 3s - loss: 1.0100 - accuracy: 0.43 - ETA: 3s - loss: 1.0101 - accuracy: 0.43 - ETA: 3s - loss: 1.0103 - accuracy: 0.43 - ETA: 3s - loss: 1.0103 - accuracy: 0.43 - ETA: 3s - loss: 1.0102 - accuracy: 0.43 - ETA: 3s - loss: 1.0102 - accuracy: 0.43 - ETA: 3s - loss: 1.0103 - accuracy: 0.43 - ETA: 3s - loss: 1.0102 - accuracy: 0.43 - ETA: 3s - loss: 1.0102 - accuracy: 0.43 - ETA: 3s - loss: 1.0101 - accuracy: 0.43 - ETA: 3s - loss: 1.0101 - accuracy: 0.43 - ETA: 3s - loss: 1.0102 - accuracy: 0.43 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0103 - accuracy: 0.44 - ETA: 2s - loss: 1.0104 - accuracy: 0.44 - ETA: 2s - loss: 1.0104 - accuracy: 0.44 - ETA: 2s - loss: 1.0104 - accuracy: 0.44 - ETA: 2s - loss: 1.0103 - accuracy: 0.44 - ETA: 2s - loss: 1.0103 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0103 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "9403/9403 [==============================] - ETA: 1s - loss: 1.0014 - accuracy: 0.65 - ETA: 8s - loss: 1.0078 - accuracy: 0.43 - ETA: 8s - loss: 1.0047 - accuracy: 0.43 - ETA: 9s - loss: 1.0083 - accuracy: 0.42 - ETA: 9s - loss: 1.0106 - accuracy: 0.42 - ETA: 10s - loss: 1.0107 - accuracy: 0.427 - ETA: 10s - loss: 1.0109 - accuracy: 0.428 - ETA: 10s - loss: 1.0089 - accuracy: 0.430 - ETA: 9s - loss: 1.0099 - accuracy: 0.433 - ETA: 9s - loss: 1.0099 - accuracy: 0.43 - ETA: 9s - loss: 1.0100 - accuracy: 0.43 - ETA: 9s - loss: 1.0098 - accuracy: 0.43 - ETA: 8s - loss: 1.0097 - accuracy: 0.43 - ETA: 9s - loss: 1.0093 - accuracy: 0.43 - ETA: 9s - loss: 1.0085 - accuracy: 0.43 - ETA: 9s - loss: 1.0089 - accuracy: 0.43 - ETA: 9s - loss: 1.0086 - accuracy: 0.43 - ETA: 9s - loss: 1.0081 - accuracy: 0.43 - ETA: 8s - loss: 1.0088 - accuracy: 0.43 - ETA: 8s - loss: 1.0105 - accuracy: 0.43 - ETA: 8s - loss: 1.0115 - accuracy: 0.43 - ETA: 8s - loss: 1.0108 - accuracy: 0.43 - ETA: 8s - loss: 1.0113 - accuracy: 0.43 - ETA: 8s - loss: 1.0110 - accuracy: 0.43 - ETA: 8s - loss: 1.0112 - accuracy: 0.43 - ETA: 8s - loss: 1.0116 - accuracy: 0.43 - ETA: 8s - loss: 1.0118 - accuracy: 0.43 - ETA: 8s - loss: 1.0117 - accuracy: 0.43 - ETA: 8s - loss: 1.0114 - accuracy: 0.43 - ETA: 8s - loss: 1.0115 - accuracy: 0.43 - ETA: 8s - loss: 1.0115 - accuracy: 0.43 - ETA: 8s - loss: 1.0114 - accuracy: 0.43 - ETA: 8s - loss: 1.0114 - accuracy: 0.43 - ETA: 8s - loss: 1.0113 - accuracy: 0.43 - ETA: 7s - loss: 1.0115 - accuracy: 0.43 - ETA: 7s - loss: 1.0115 - accuracy: 0.43 - ETA: 7s - loss: 1.0117 - accuracy: 0.43 - ETA: 7s - loss: 1.0115 - accuracy: 0.43 - ETA: 7s - loss: 1.0114 - accuracy: 0.43 - ETA: 7s - loss: 1.0112 - accuracy: 0.43 - ETA: 7s - loss: 1.0109 - accuracy: 0.43 - ETA: 7s - loss: 1.0112 - accuracy: 0.43 - ETA: 7s - loss: 1.0112 - accuracy: 0.43 - ETA: 7s - loss: 1.0109 - accuracy: 0.43 - ETA: 7s - loss: 1.0112 - accuracy: 0.43 - ETA: 7s - loss: 1.0114 - accuracy: 0.43 - ETA: 7s - loss: 1.0110 - accuracy: 0.43 - ETA: 7s - loss: 1.0109 - accuracy: 0.43 - ETA: 7s - loss: 1.0107 - accuracy: 0.43 - ETA: 7s - loss: 1.0105 - accuracy: 0.43 - ETA: 6s - loss: 1.0106 - accuracy: 0.43 - ETA: 6s - loss: 1.0102 - accuracy: 0.43 - ETA: 6s - loss: 1.0102 - accuracy: 0.43 - ETA: 6s - loss: 1.0104 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.43 - ETA: 6s - loss: 1.0107 - accuracy: 0.43 - ETA: 6s - loss: 1.0111 - accuracy: 0.43 - ETA: 6s - loss: 1.0112 - accuracy: 0.43 - ETA: 6s - loss: 1.0113 - accuracy: 0.43 - ETA: 6s - loss: 1.0112 - accuracy: 0.43 - ETA: 6s - loss: 1.0111 - accuracy: 0.43 - ETA: 6s - loss: 1.0112 - accuracy: 0.43 - ETA: 5s - loss: 1.0114 - accuracy: 0.43 - ETA: 5s - loss: 1.0111 - accuracy: 0.43 - ETA: 5s - loss: 1.0108 - accuracy: 0.43 - ETA: 5s - loss: 1.0106 - accuracy: 0.43 - ETA: 5s - loss: 1.0106 - accuracy: 0.43 - ETA: 5s - loss: 1.0108 - accuracy: 0.43 - ETA: 5s - loss: 1.0106 - accuracy: 0.43 - ETA: 5s - loss: 1.0104 - accuracy: 0.43 - ETA: 5s - loss: 1.0103 - accuracy: 0.43 - ETA: 5s - loss: 1.0103 - accuracy: 0.43 - ETA: 5s - loss: 1.0102 - accuracy: 0.43 - ETA: 5s - loss: 1.0100 - accuracy: 0.43 - ETA: 5s - loss: 1.0099 - accuracy: 0.43 - ETA: 5s - loss: 1.0099 - accuracy: 0.43 - ETA: 5s - loss: 1.0099 - accuracy: 0.43 - ETA: 5s - loss: 1.0098 - accuracy: 0.43 - ETA: 5s - loss: 1.0097 - accuracy: 0.43 - ETA: 5s - loss: 1.0096 - accuracy: 0.43 - ETA: 5s - loss: 1.0096 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.43 - ETA: 4s - loss: 1.0096 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.43 - ETA: 4s - loss: 1.0094 - accuracy: 0.43 - ETA: 4s - loss: 1.0094 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.43 - ETA: 4s - loss: 1.0094 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.44 - ETA: 4s - loss: 1.0095 - accuracy: 0.44 - ETA: 4s - loss: 1.0095 - accuracy: 0.43 - ETA: 4s - loss: 1.0096 - accuracy: 0.43 - ETA: 4s - loss: 1.0096 - accuracy: 0.43 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.43 - ETA: 4s - loss: 1.0095 - accuracy: 0.44 - ETA: 4s - loss: 1.0094 - accuracy: 0.44 - ETA: 4s - loss: 1.0094 - accuracy: 0.44 - ETA: 4s - loss: 1.0095 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.43 - ETA: 3s - loss: 1.0094 - accuracy: 0.43 - ETA: 3s - loss: 1.0095 - accuracy: 0.43 - ETA: 3s - loss: 1.0094 - accuracy: 0.43 - ETA: 3s - loss: 1.0094 - accuracy: 0.43 - ETA: 3s - loss: 1.0094 - accuracy: 0.43 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.43 - ETA: 3s - loss: 1.0097 - accuracy: 0.43 - ETA: 3s - loss: 1.0097 - accuracy: 0.43 - ETA: 3s - loss: 1.0097 - accuracy: 0.44 - ETA: 3s - loss: 1.0097 - accuracy: 0.44 - ETA: 3s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0091 - accuracy: 0.44 - ETA: 2s - loss: 1.0091 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0091 - accuracy: 0.44 - ETA: 2s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0090 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 0.9836 - accuracy: 0.46 - ETA: 9s - loss: 1.0102 - accuracy: 0.43 - ETA: 8s - loss: 1.0154 - accuracy: 0.43 - ETA: 8s - loss: 1.0121 - accuracy: 0.43 - ETA: 8s - loss: 1.0083 - accuracy: 0.43 - ETA: 8s - loss: 1.0063 - accuracy: 0.44 - ETA: 8s - loss: 1.0068 - accuracy: 0.44 - ETA: 8s - loss: 1.0065 - accuracy: 0.44 - ETA: 8s - loss: 1.0077 - accuracy: 0.44 - ETA: 8s - loss: 1.0072 - accuracy: 0.44 - ETA: 8s - loss: 1.0088 - accuracy: 0.44 - ETA: 8s - loss: 1.0085 - accuracy: 0.44 - ETA: 8s - loss: 1.0079 - accuracy: 0.44 - ETA: 7s - loss: 1.0071 - accuracy: 0.44 - ETA: 7s - loss: 1.0063 - accuracy: 0.44 - ETA: 7s - loss: 1.0067 - accuracy: 0.44 - ETA: 7s - loss: 1.0075 - accuracy: 0.44 - ETA: 7s - loss: 1.0085 - accuracy: 0.44 - ETA: 7s - loss: 1.0083 - accuracy: 0.44 - ETA: 7s - loss: 1.0085 - accuracy: 0.44 - ETA: 7s - loss: 1.0090 - accuracy: 0.44 - ETA: 7s - loss: 1.0089 - accuracy: 0.44 - ETA: 7s - loss: 1.0084 - accuracy: 0.44 - ETA: 7s - loss: 1.0093 - accuracy: 0.44 - ETA: 7s - loss: 1.0088 - accuracy: 0.44 - ETA: 7s - loss: 1.0090 - accuracy: 0.44 - ETA: 7s - loss: 1.0093 - accuracy: 0.44 - ETA: 7s - loss: 1.0092 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 6s - loss: 1.0095 - accuracy: 0.44 - ETA: 6s - loss: 1.0089 - accuracy: 0.44 - ETA: 6s - loss: 1.0088 - accuracy: 0.44 - ETA: 6s - loss: 1.0088 - accuracy: 0.44 - ETA: 6s - loss: 1.0087 - accuracy: 0.44 - ETA: 6s - loss: 1.0090 - accuracy: 0.44 - ETA: 6s - loss: 1.0090 - accuracy: 0.44 - ETA: 7s - loss: 1.0088 - accuracy: 0.44 - ETA: 7s - loss: 1.0091 - accuracy: 0.44 - ETA: 7s - loss: 1.0092 - accuracy: 0.44 - ETA: 6s - loss: 1.0090 - accuracy: 0.44 - ETA: 6s - loss: 1.0088 - accuracy: 0.44 - ETA: 6s - loss: 1.0089 - accuracy: 0.44 - ETA: 6s - loss: 1.0089 - accuracy: 0.44 - ETA: 6s - loss: 1.0089 - accuracy: 0.44 - ETA: 6s - loss: 1.0088 - accuracy: 0.44 - ETA: 6s - loss: 1.0085 - accuracy: 0.44 - ETA: 6s - loss: 1.0086 - accuracy: 0.44 - ETA: 6s - loss: 1.0089 - accuracy: 0.44 - ETA: 6s - loss: 1.0090 - accuracy: 0.44 - ETA: 6s - loss: 1.0090 - accuracy: 0.44 - ETA: 6s - loss: 1.0093 - accuracy: 0.44 - ETA: 6s - loss: 1.0092 - accuracy: 0.44 - ETA: 6s - loss: 1.0092 - accuracy: 0.44 - ETA: 6s - loss: 1.0088 - accuracy: 0.44 - ETA: 6s - loss: 1.0086 - accuracy: 0.44 - ETA: 6s - loss: 1.0084 - accuracy: 0.44 - ETA: 6s - loss: 1.0087 - accuracy: 0.44 - ETA: 6s - loss: 1.0084 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0085 - accuracy: 0.44 - ETA: 5s - loss: 1.0085 - accuracy: 0.44 - ETA: 5s - loss: 1.0084 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0080 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0078 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0078 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 5s - loss: 1.0079 - accuracy: 0.44 - ETA: 5s - loss: 1.0081 - accuracy: 0.44 - ETA: 4s - loss: 1.0082 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0086 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0083 - accuracy: 0.44 - ETA: 4s - loss: 1.0083 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0085 - accuracy: 0.44 - ETA: 4s - loss: 1.0084 - accuracy: 0.44 - ETA: 4s - loss: 1.0083 - accuracy: 0.44 - ETA: 4s - loss: 1.0082 - accuracy: 0.44 - ETA: 3s - loss: 1.0084 - accuracy: 0.44 - ETA: 3s - loss: 1.0084 - accuracy: 0.44 - ETA: 3s - loss: 1.0085 - accuracy: 0.44 - ETA: 3s - loss: 1.0085 - accuracy: 0.44 - ETA: 3s - loss: 1.0085 - accuracy: 0.44 - ETA: 3s - loss: 1.0086 - accuracy: 0.44 - ETA: 3s - loss: 1.0087 - accuracy: 0.44 - ETA: 3s - loss: 1.0087 - accuracy: 0.44 - ETA: 3s - loss: 1.0088 - accuracy: 0.44 - ETA: 3s - loss: 1.0088 - accuracy: 0.44 - ETA: 3s - loss: 1.0088 - accuracy: 0.44 - ETA: 3s - loss: 1.0089 - accuracy: 0.44 - ETA: 3s - loss: 1.0090 - accuracy: 0.44 - ETA: 3s - loss: 1.0090 - accuracy: 0.44 - ETA: 3s - loss: 1.0090 - accuracy: 0.44 - ETA: 3s - loss: 1.0091 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0090 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 0.9839 - accuracy: 0.46 - ETA: 9s - loss: 0.9968 - accuracy: 0.47 - ETA: 8s - loss: 1.0085 - accuracy: 0.46 - ETA: 9s - loss: 1.0123 - accuracy: 0.45 - ETA: 9s - loss: 1.0100 - accuracy: 0.44 - ETA: 9s - loss: 1.0116 - accuracy: 0.45 - ETA: 9s - loss: 1.0117 - accuracy: 0.45 - ETA: 8s - loss: 1.0120 - accuracy: 0.45 - ETA: 8s - loss: 1.0118 - accuracy: 0.45 - ETA: 8s - loss: 1.0109 - accuracy: 0.45 - ETA: 9s - loss: 1.0111 - accuracy: 0.45 - ETA: 9s - loss: 1.0109 - accuracy: 0.45 - ETA: 9s - loss: 1.0122 - accuracy: 0.45 - ETA: 9s - loss: 1.0113 - accuracy: 0.44 - ETA: 8s - loss: 1.0110 - accuracy: 0.44 - ETA: 8s - loss: 1.0106 - accuracy: 0.44 - ETA: 8s - loss: 1.0105 - accuracy: 0.44 - ETA: 8s - loss: 1.0096 - accuracy: 0.44 - ETA: 8s - loss: 1.0099 - accuracy: 0.44 - ETA: 8s - loss: 1.0103 - accuracy: 0.44 - ETA: 8s - loss: 1.0104 - accuracy: 0.44 - ETA: 8s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0111 - accuracy: 0.44 - ETA: 7s - loss: 1.0106 - accuracy: 0.44 - ETA: 7s - loss: 1.0111 - accuracy: 0.44 - ETA: 8s - loss: 1.0110 - accuracy: 0.44 - ETA: 8s - loss: 1.0113 - accuracy: 0.44 - ETA: 8s - loss: 1.0117 - accuracy: 0.44 - ETA: 8s - loss: 1.0112 - accuracy: 0.44 - ETA: 8s - loss: 1.0110 - accuracy: 0.44 - ETA: 8s - loss: 1.0106 - accuracy: 0.44 - ETA: 8s - loss: 1.0104 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0095 - accuracy: 0.44 - ETA: 8s - loss: 1.0094 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0094 - accuracy: 0.44 - ETA: 8s - loss: 1.0096 - accuracy: 0.44 - ETA: 8s - loss: 1.0097 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.44 - ETA: 8s - loss: 1.0099 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0097 - accuracy: 0.44 - ETA: 8s - loss: 1.0096 - accuracy: 0.44 - ETA: 8s - loss: 1.0097 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0097 - accuracy: 0.44 - ETA: 8s - loss: 1.0097 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0103 - accuracy: 0.44 - ETA: 7s - loss: 1.0104 - accuracy: 0.44 - ETA: 7s - loss: 1.0103 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 6s - loss: 1.0100 - accuracy: 0.44 - ETA: 6s - loss: 1.0101 - accuracy: 0.44 - ETA: 6s - loss: 1.0098 - accuracy: 0.44 - ETA: 6s - loss: 1.0097 - accuracy: 0.44 - ETA: 6s - loss: 1.0101 - accuracy: 0.44 - ETA: 6s - loss: 1.0101 - accuracy: 0.44 - ETA: 6s - loss: 1.0104 - accuracy: 0.44 - ETA: 6s - loss: 1.0103 - accuracy: 0.44 - ETA: 6s - loss: 1.0102 - accuracy: 0.44 - ETA: 6s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0098 - accuracy: 0.44 - ETA: 5s - loss: 1.0097 - accuracy: 0.44 - ETA: 5s - loss: 1.0097 - accuracy: 0.44 - ETA: 5s - loss: 1.0097 - accuracy: 0.44 - ETA: 5s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0095 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0098 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0097 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 4s - loss: 1.0096 - accuracy: 0.44 - ETA: 3s - loss: 1.0096 - accuracy: 0.44 - ETA: 3s - loss: 1.0096 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0096 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9153/9403 [============================>.] - ETA: 0s - loss: 0.9518 - accuracy: 0.46 - ETA: 14s - loss: 0.9864 - accuracy: 0.446 - ETA: 15s - loss: 1.0010 - accuracy: 0.443 - ETA: 15s - loss: 1.0025 - accuracy: 0.444 - ETA: 15s - loss: 1.0009 - accuracy: 0.444 - ETA: 14s - loss: 1.0042 - accuracy: 0.446 - ETA: 13s - loss: 1.0008 - accuracy: 0.451 - ETA: 12s - loss: 1.0027 - accuracy: 0.451 - ETA: 11s - loss: 1.0030 - accuracy: 0.454 - ETA: 11s - loss: 1.0001 - accuracy: 0.452 - ETA: 11s - loss: 1.0000 - accuracy: 0.451 - ETA: 11s - loss: 1.0015 - accuracy: 0.448 - ETA: 11s - loss: 1.0023 - accuracy: 0.449 - ETA: 10s - loss: 1.0022 - accuracy: 0.449 - ETA: 10s - loss: 1.0025 - accuracy: 0.450 - ETA: 10s - loss: 1.0031 - accuracy: 0.449 - ETA: 10s - loss: 1.0034 - accuracy: 0.448 - ETA: 10s - loss: 1.0038 - accuracy: 0.448 - ETA: 10s - loss: 1.0045 - accuracy: 0.447 - ETA: 10s - loss: 1.0051 - accuracy: 0.448 - ETA: 10s - loss: 1.0044 - accuracy: 0.448 - ETA: 9s - loss: 1.0044 - accuracy: 0.447 - ETA: 9s - loss: 1.0045 - accuracy: 0.44 - ETA: 9s - loss: 1.0044 - accuracy: 0.44 - ETA: 9s - loss: 1.0044 - accuracy: 0.44 - ETA: 9s - loss: 1.0043 - accuracy: 0.44 - ETA: 9s - loss: 1.0049 - accuracy: 0.44 - ETA: 9s - loss: 1.0043 - accuracy: 0.44 - ETA: 9s - loss: 1.0041 - accuracy: 0.44 - ETA: 9s - loss: 1.0041 - accuracy: 0.44 - ETA: 9s - loss: 1.0039 - accuracy: 0.44 - ETA: 9s - loss: 1.0042 - accuracy: 0.44 - ETA: 8s - loss: 1.0048 - accuracy: 0.44 - ETA: 8s - loss: 1.0052 - accuracy: 0.44 - ETA: 8s - loss: 1.0053 - accuracy: 0.44 - ETA: 8s - loss: 1.0053 - accuracy: 0.44 - ETA: 8s - loss: 1.0054 - accuracy: 0.44 - ETA: 8s - loss: 1.0055 - accuracy: 0.44 - ETA: 8s - loss: 1.0058 - accuracy: 0.44 - ETA: 8s - loss: 1.0059 - accuracy: 0.44 - ETA: 8s - loss: 1.0059 - accuracy: 0.44 - ETA: 8s - loss: 1.0058 - accuracy: 0.44 - ETA: 8s - loss: 1.0061 - accuracy: 0.44 - ETA: 8s - loss: 1.0063 - accuracy: 0.44 - ETA: 8s - loss: 1.0061 - accuracy: 0.44 - ETA: 8s - loss: 1.0062 - accuracy: 0.44 - ETA: 8s - loss: 1.0064 - accuracy: 0.44 - ETA: 8s - loss: 1.0065 - accuracy: 0.44 - ETA: 8s - loss: 1.0067 - accuracy: 0.44 - ETA: 8s - loss: 1.0065 - accuracy: 0.44 - ETA: 8s - loss: 1.0065 - accuracy: 0.44 - ETA: 8s - loss: 1.0069 - accuracy: 0.44 - ETA: 8s - loss: 1.0071 - accuracy: 0.44 - ETA: 8s - loss: 1.0070 - accuracy: 0.44 - ETA: 7s - loss: 1.0070 - accuracy: 0.44 - ETA: 7s - loss: 1.0071 - accuracy: 0.44 - ETA: 7s - loss: 1.0070 - accuracy: 0.44 - ETA: 7s - loss: 1.0071 - accuracy: 0.44 - ETA: 7s - loss: 1.0071 - accuracy: 0.44 - ETA: 7s - loss: 1.0070 - accuracy: 0.44 - ETA: 7s - loss: 1.0071 - accuracy: 0.44 - ETA: 7s - loss: 1.0072 - accuracy: 0.44 - ETA: 7s - loss: 1.0070 - accuracy: 0.44 - ETA: 7s - loss: 1.0069 - accuracy: 0.44 - ETA: 7s - loss: 1.0072 - accuracy: 0.44 - ETA: 7s - loss: 1.0073 - accuracy: 0.44 - ETA: 7s - loss: 1.0074 - accuracy: 0.44 - ETA: 7s - loss: 1.0078 - accuracy: 0.44 - ETA: 7s - loss: 1.0076 - accuracy: 0.44 - ETA: 7s - loss: 1.0075 - accuracy: 0.44 - ETA: 7s - loss: 1.0074 - accuracy: 0.44 - ETA: 7s - loss: 1.0075 - accuracy: 0.44 - ETA: 6s - loss: 1.0079 - accuracy: 0.44 - ETA: 6s - loss: 1.0079 - accuracy: 0.44 - ETA: 6s - loss: 1.0081 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0081 - accuracy: 0.44 - ETA: 6s - loss: 1.0083 - accuracy: 0.44 - ETA: 6s - loss: 1.0083 - accuracy: 0.44 - ETA: 6s - loss: 1.0080 - accuracy: 0.44 - ETA: 6s - loss: 1.0081 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0080 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 6s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0084 - accuracy: 0.44 - ETA: 5s - loss: 1.0085 - accuracy: 0.44 - ETA: 5s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0082 - accuracy: 0.44 - ETA: 5s - loss: 1.0083 - accuracy: 0.44 - ETA: 5s - loss: 1.0086 - accuracy: 0.44 - ETA: 5s - loss: 1.0086 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0087 - accuracy: 0.44 - ETA: 4s - loss: 1.0086 - accuracy: 0.44 - ETA: 4s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0087 - accuracy: 0.44 - ETA: 4s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0089 - accuracy: 0.44 - ETA: 4s - loss: 1.0089 - accuracy: 0.44 - ETA: 4s - loss: 1.0090 - accuracy: 0.44 - ETA: 4s - loss: 1.0092 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0092 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0092 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0096 - accuracy: 0.44 - ETA: 3s - loss: 1.0096 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0091 - accuracy: 0.44 - ETA: 3s - loss: 1.0090 - accuracy: 0.44 - ETA: 3s - loss: 1.0090 - accuracy: 0.44 - ETA: 3s - loss: 1.0089 - accuracy: 0.44 - ETA: 3s - loss: 1.0089 - accuracy: 0.44 - ETA: 2s - loss: 1.0090 - accuracy: 0.44 - ETA: 2s - loss: 1.0089 - accuracy: 0.44 - ETA: 2s - loss: 1.0088 - accuracy: 0.44 - ETA: 2s - loss: 1.0088 - accuracy: 0.44 - ETA: 2s - loss: 1.0087 - accuracy: 0.44 - ETA: 2s - loss: 1.0088 - accuracy: 0.44 - ETA: 2s - loss: 1.0087 - accuracy: 0.44 - ETA: 2s - loss: 1.0088 - accuracy: 0.44 - ETA: 2s - loss: 1.0087 - accuracy: 0.44 - ETA: 2s - loss: 1.0087 - accuracy: 0.44 - ETA: 2s - loss: 1.0088 - accuracy: 0.44 - ETA: 2s - loss: 1.0087 - accuracy: 0.44 - ETA: 2s - loss: 1.0088 - accuracy: 0.44 - ETA: 2s - loss: 1.0089 - accuracy: 0.44 - ETA: 2s - loss: 1.0089 - accuracy: 0.44 - ETA: 2s - loss: 1.0090 - accuracy: 0.44 - ETA: 2s - loss: 1.0090 - accuracy: 0.44 - ETA: 2s - loss: 1.0090 - accuracy: 0.44 - ETA: 2s - loss: 1.0090 - accuracy: 0.44 - ETA: 2s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0090 - accuracy: 0.44 - ETA: 1s - loss: 1.0089 - accuracy: 0.44 - ETA: 1s - loss: 1.0090 - accuracy: 0.44 - ETA: 1s - loss: 1.0090 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.4415"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0156 - accuracy: 0.46 - ETA: 7s - loss: 1.0042 - accuracy: 0.43 - ETA: 10s - loss: 1.0107 - accuracy: 0.432 - ETA: 9s - loss: 1.0147 - accuracy: 0.439 - ETA: 9s - loss: 1.0188 - accuracy: 0.43 - ETA: 8s - loss: 1.0153 - accuracy: 0.44 - ETA: 8s - loss: 1.0146 - accuracy: 0.44 - ETA: 8s - loss: 1.0136 - accuracy: 0.43 - ETA: 8s - loss: 1.0135 - accuracy: 0.44 - ETA: 8s - loss: 1.0137 - accuracy: 0.44 - ETA: 7s - loss: 1.0132 - accuracy: 0.44 - ETA: 7s - loss: 1.0115 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0109 - accuracy: 0.44 - ETA: 7s - loss: 1.0115 - accuracy: 0.44 - ETA: 7s - loss: 1.0118 - accuracy: 0.44 - ETA: 7s - loss: 1.0115 - accuracy: 0.44 - ETA: 7s - loss: 1.0113 - accuracy: 0.44 - ETA: 7s - loss: 1.0112 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 7s - loss: 1.0106 - accuracy: 0.44 - ETA: 7s - loss: 1.0111 - accuracy: 0.44 - ETA: 7s - loss: 1.0107 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0107 - accuracy: 0.43 - ETA: 7s - loss: 1.0099 - accuracy: 0.43 - ETA: 7s - loss: 1.0101 - accuracy: 0.43 - ETA: 7s - loss: 1.0101 - accuracy: 0.43 - ETA: 6s - loss: 1.0101 - accuracy: 0.43 - ETA: 6s - loss: 1.0103 - accuracy: 0.43 - ETA: 6s - loss: 1.0105 - accuracy: 0.43 - ETA: 6s - loss: 1.0098 - accuracy: 0.43 - ETA: 6s - loss: 1.0099 - accuracy: 0.43 - ETA: 6s - loss: 1.0096 - accuracy: 0.43 - ETA: 6s - loss: 1.0097 - accuracy: 0.43 - ETA: 6s - loss: 1.0094 - accuracy: 0.43 - ETA: 6s - loss: 1.0095 - accuracy: 0.43 - ETA: 6s - loss: 1.0095 - accuracy: 0.43 - ETA: 6s - loss: 1.0096 - accuracy: 0.43 - ETA: 6s - loss: 1.0097 - accuracy: 0.43 - ETA: 6s - loss: 1.0094 - accuracy: 0.43 - ETA: 6s - loss: 1.0096 - accuracy: 0.43 - ETA: 6s - loss: 1.0096 - accuracy: 0.43 - ETA: 6s - loss: 1.0095 - accuracy: 0.43 - ETA: 6s - loss: 1.0093 - accuracy: 0.43 - ETA: 6s - loss: 1.0090 - accuracy: 0.44 - ETA: 6s - loss: 1.0093 - accuracy: 0.44 - ETA: 6s - loss: 1.0095 - accuracy: 0.43 - ETA: 6s - loss: 1.0095 - accuracy: 0.43 - ETA: 5s - loss: 1.0093 - accuracy: 0.43 - ETA: 5s - loss: 1.0093 - accuracy: 0.43 - ETA: 5s - loss: 1.0090 - accuracy: 0.43 - ETA: 5s - loss: 1.0090 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0088 - accuracy: 0.43 - ETA: 5s - loss: 1.0087 - accuracy: 0.43 - ETA: 5s - loss: 1.0087 - accuracy: 0.43 - ETA: 5s - loss: 1.0087 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0088 - accuracy: 0.43 - ETA: 5s - loss: 1.0088 - accuracy: 0.43 - ETA: 5s - loss: 1.0088 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0090 - accuracy: 0.43 - ETA: 5s - loss: 1.0090 - accuracy: 0.43 - ETA: 5s - loss: 1.0092 - accuracy: 0.43 - ETA: 5s - loss: 1.0089 - accuracy: 0.43 - ETA: 5s - loss: 1.0088 - accuracy: 0.44 - ETA: 5s - loss: 1.0090 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0088 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0090 - accuracy: 0.44 - ETA: 5s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0089 - accuracy: 0.44 - ETA: 4s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0089 - accuracy: 0.44 - ETA: 5s - loss: 1.0090 - accuracy: 0.44 - ETA: 5s - loss: 1.0090 - accuracy: 0.44 - ETA: 5s - loss: 1.0088 - accuracy: 0.44 - ETA: 5s - loss: 1.0088 - accuracy: 0.44 - ETA: 4s - loss: 1.0089 - accuracy: 0.44 - ETA: 4s - loss: 1.0089 - accuracy: 0.44 - ETA: 4s - loss: 1.0091 - accuracy: 0.43 - ETA: 4s - loss: 1.0092 - accuracy: 0.43 - ETA: 4s - loss: 1.0092 - accuracy: 0.43 - ETA: 4s - loss: 1.0093 - accuracy: 0.43 - ETA: 4s - loss: 1.0093 - accuracy: 0.43 - ETA: 4s - loss: 1.0094 - accuracy: 0.43 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0093 - accuracy: 0.44 - ETA: 4s - loss: 1.0092 - accuracy: 0.44 - ETA: 4s - loss: 1.0091 - accuracy: 0.44 - ETA: 4s - loss: 1.0091 - accuracy: 0.44 - ETA: 4s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0091 - accuracy: 0.44 - ETA: 3s - loss: 1.0091 - accuracy: 0.44 - ETA: 3s - loss: 1.0091 - accuracy: 0.44 - ETA: 3s - loss: 1.0090 - accuracy: 0.44 - ETA: 3s - loss: 1.0091 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0093 - accuracy: 0.44 - ETA: 3s - loss: 1.0092 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0094 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 3s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0097 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0095 - accuracy: 0.44 - ETA: 2s - loss: 1.0096 - accuracy: 0.44 - ETA: 2s - loss: 1.0094 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0092 - accuracy: 0.44 - ETA: 2s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0093 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0091 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 1s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0090 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0091 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0092 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 0.9862 - accuracy: 0.43 - ETA: 7s - loss: 1.0125 - accuracy: 0.45 - ETA: 7s - loss: 1.0137 - accuracy: 0.44 - ETA: 7s - loss: 1.0128 - accuracy: 0.43 - ETA: 7s - loss: 1.0107 - accuracy: 0.43 - ETA: 7s - loss: 1.0115 - accuracy: 0.43 - ETA: 7s - loss: 1.0103 - accuracy: 0.43 - ETA: 7s - loss: 1.0095 - accuracy: 0.43 - ETA: 7s - loss: 1.0091 - accuracy: 0.43 - ETA: 7s - loss: 1.0091 - accuracy: 0.43 - ETA: 7s - loss: 1.0084 - accuracy: 0.43 - ETA: 7s - loss: 1.0085 - accuracy: 0.43 - ETA: 7s - loss: 1.0094 - accuracy: 0.43 - ETA: 7s - loss: 1.0092 - accuracy: 0.43 - ETA: 7s - loss: 1.0089 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.43 - ETA: 7s - loss: 1.0103 - accuracy: 0.44 - ETA: 7s - loss: 1.0103 - accuracy: 0.44 - ETA: 8s - loss: 1.0103 - accuracy: 0.44 - ETA: 8s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0104 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0115 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0115 - accuracy: 0.44 - ETA: 7s - loss: 1.0114 - accuracy: 0.43 - ETA: 7s - loss: 1.0115 - accuracy: 0.43 - ETA: 7s - loss: 1.0112 - accuracy: 0.43 - ETA: 7s - loss: 1.0109 - accuracy: 0.43 - ETA: 7s - loss: 1.0104 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0094 - accuracy: 0.44 - ETA: 7s - loss: 1.0089 - accuracy: 0.44 - ETA: 7s - loss: 1.0087 - accuracy: 0.44 - ETA: 7s - loss: 1.0092 - accuracy: 0.44 - ETA: 7s - loss: 1.0092 - accuracy: 0.44 - ETA: 7s - loss: 1.0094 - accuracy: 0.44 - ETA: 7s - loss: 1.0096 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0103 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 8s - loss: 1.0103 - accuracy: 0.44 - ETA: 8s - loss: 1.0099 - accuracy: 0.44 - ETA: 8s - loss: 1.0099 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0098 - accuracy: 0.44 - ETA: 8s - loss: 1.0100 - accuracy: 0.44 - ETA: 8s - loss: 1.0101 - accuracy: 0.44 - ETA: 8s - loss: 1.0102 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 7s - loss: 1.0104 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 7s - loss: 1.0103 - accuracy: 0.44 - ETA: 7s - loss: 1.0104 - accuracy: 0.44 - ETA: 7s - loss: 1.0107 - accuracy: 0.44 - ETA: 7s - loss: 1.0108 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0108 - accuracy: 0.44 - ETA: 7s - loss: 1.0111 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0110 - accuracy: 0.44 - ETA: 7s - loss: 1.0113 - accuracy: 0.44 - ETA: 6s - loss: 1.0114 - accuracy: 0.44 - ETA: 6s - loss: 1.0114 - accuracy: 0.44 - ETA: 6s - loss: 1.0116 - accuracy: 0.44 - ETA: 6s - loss: 1.0115 - accuracy: 0.44 - ETA: 6s - loss: 1.0115 - accuracy: 0.44 - ETA: 6s - loss: 1.0113 - accuracy: 0.44 - ETA: 6s - loss: 1.0113 - accuracy: 0.44 - ETA: 6s - loss: 1.0112 - accuracy: 0.44 - ETA: 6s - loss: 1.0112 - accuracy: 0.44 - ETA: 6s - loss: 1.0112 - accuracy: 0.44 - ETA: 6s - loss: 1.0113 - accuracy: 0.44 - ETA: 6s - loss: 1.0112 - accuracy: 0.44 - ETA: 6s - loss: 1.0110 - accuracy: 0.44 - ETA: 6s - loss: 1.0108 - accuracy: 0.44 - ETA: 6s - loss: 1.0109 - accuracy: 0.44 - ETA: 6s - loss: 1.0108 - accuracy: 0.44 - ETA: 5s - loss: 1.0108 - accuracy: 0.44 - ETA: 5s - loss: 1.0107 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0107 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0107 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.44 - ETA: 5s - loss: 1.0106 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0105 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0104 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 5s - loss: 1.0103 - accuracy: 0.44 - ETA: 4s - loss: 1.0104 - accuracy: 0.44 - ETA: 4s - loss: 1.0104 - accuracy: 0.44 - ETA: 4s - loss: 1.0105 - accuracy: 0.44 - ETA: 4s - loss: 1.0106 - accuracy: 0.44 - ETA: 4s - loss: 1.0105 - accuracy: 0.44 - ETA: 4s - loss: 1.0106 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0107 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0108 - accuracy: 0.44 - ETA: 4s - loss: 1.0107 - accuracy: 0.44 - ETA: 4s - loss: 1.0109 - accuracy: 0.44 - ETA: 3s - loss: 1.0107 - accuracy: 0.44 - ETA: 3s - loss: 1.0106 - accuracy: 0.44 - ETA: 3s - loss: 1.0106 - accuracy: 0.44 - ETA: 3s - loss: 1.0106 - accuracy: 0.44 - ETA: 3s - loss: 1.0106 - accuracy: 0.44 - ETA: 3s - loss: 1.0106 - accuracy: 0.44 - ETA: 3s - loss: 1.0104 - accuracy: 0.44 - ETA: 3s - loss: 1.0104 - accuracy: 0.44 - ETA: 3s - loss: 1.0104 - accuracy: 0.44 - ETA: 3s - loss: 1.0105 - accuracy: 0.44 - ETA: 3s - loss: 1.0104 - accuracy: 0.44 - ETA: 3s - loss: 1.0103 - accuracy: 0.44 - ETA: 3s - loss: 1.0103 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0103 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0094 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0242 - accuracy: 0.34 - ETA: 7s - loss: 1.0016 - accuracy: 0.45 - ETA: 7s - loss: 0.9997 - accuracy: 0.45 - ETA: 7s - loss: 1.0028 - accuracy: 0.45 - ETA: 9s - loss: 1.0034 - accuracy: 0.45 - ETA: 8s - loss: 1.0014 - accuracy: 0.45 - ETA: 8s - loss: 1.0028 - accuracy: 0.44 - ETA: 8s - loss: 1.0044 - accuracy: 0.44 - ETA: 8s - loss: 1.0033 - accuracy: 0.44 - ETA: 8s - loss: 1.0047 - accuracy: 0.44 - ETA: 8s - loss: 1.0053 - accuracy: 0.44 - ETA: 8s - loss: 1.0057 - accuracy: 0.44 - ETA: 8s - loss: 1.0066 - accuracy: 0.44 - ETA: 8s - loss: 1.0071 - accuracy: 0.44 - ETA: 8s - loss: 1.0068 - accuracy: 0.44 - ETA: 8s - loss: 1.0070 - accuracy: 0.44 - ETA: 8s - loss: 1.0076 - accuracy: 0.44 - ETA: 8s - loss: 1.0072 - accuracy: 0.44 - ETA: 8s - loss: 1.0076 - accuracy: 0.44 - ETA: 8s - loss: 1.0077 - accuracy: 0.44 - ETA: 8s - loss: 1.0084 - accuracy: 0.44 - ETA: 8s - loss: 1.0081 - accuracy: 0.44 - ETA: 8s - loss: 1.0087 - accuracy: 0.44 - ETA: 8s - loss: 1.0093 - accuracy: 0.44 - ETA: 7s - loss: 1.0091 - accuracy: 0.44 - ETA: 7s - loss: 1.0090 - accuracy: 0.44 - ETA: 7s - loss: 1.0089 - accuracy: 0.44 - ETA: 7s - loss: 1.0091 - accuracy: 0.44 - ETA: 7s - loss: 1.0093 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0094 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0094 - accuracy: 0.44 - ETA: 7s - loss: 1.0094 - accuracy: 0.44 - ETA: 7s - loss: 1.0095 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0098 - accuracy: 0.44 - ETA: 7s - loss: 1.0097 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0099 - accuracy: 0.44 - ETA: 7s - loss: 1.0101 - accuracy: 0.44 - ETA: 7s - loss: 1.0100 - accuracy: 0.44 - ETA: 7s - loss: 1.0102 - accuracy: 0.44 - ETA: 6s - loss: 1.0100 - accuracy: 0.44 - ETA: 6s - loss: 1.0101 - accuracy: 0.44 - ETA: 6s - loss: 1.0103 - accuracy: 0.44 - ETA: 6s - loss: 1.0105 - accuracy: 0.44 - ETA: 6s - loss: 1.0104 - accuracy: 0.44 - ETA: 6s - loss: 1.0105 - accuracy: 0.44 - ETA: 6s - loss: 1.0106 - accuracy: 0.44 - ETA: 6s - loss: 1.0105 - accuracy: 0.44 - ETA: 6s - loss: 1.0107 - accuracy: 0.44 - ETA: 6s - loss: 1.0107 - accuracy: 0.44 - ETA: 6s - loss: 1.0106 - accuracy: 0.44 - ETA: 6s - loss: 1.0106 - accuracy: 0.44 - ETA: 6s - loss: 1.0106 - accuracy: 0.44 - ETA: 6s - loss: 1.0105 - accuracy: 0.44 - ETA: 6s - loss: 1.0104 - accuracy: 0.44 - ETA: 6s - loss: 1.0102 - accuracy: 0.44 - ETA: 6s - loss: 1.0102 - accuracy: 0.44 - ETA: 6s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0099 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0102 - accuracy: 0.43 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0100 - accuracy: 0.44 - ETA: 5s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0102 - accuracy: 0.44 - ETA: 4s - loss: 1.0103 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0101 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0099 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 4s - loss: 1.0100 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0102 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 3s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0102 - accuracy: 0.44 - ETA: 2s - loss: 1.0101 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0100 - accuracy: 0.44 - ETA: 2s - loss: 1.0099 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 2s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0098 - accuracy: 0.44 - ETA: 1s - loss: 1.0097 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0096 - accuracy: 0.44 - ETA: 1s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0096 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0095 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0094 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - ETA: 0s - loss: 1.0093 - accuracy: 0.44 - 11s 1ms/step - loss: 1.0093 - accuracy: 0.4414 - val_loss: 1.0093 - val_accuracy: 0.4414\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: ef887045e4cb0ee2fbabeeedf3249fba</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.4413926899433136</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 7</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_4: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_5: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_6: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9403/9403 [==============================] - ETA: 0s - loss: 1.1238 - accuracy: 0.03 - ETA: 7s - loss: 1.2315 - accuracy: 0.42 - ETA: 7s - loss: 1.1398 - accuracy: 0.42 - ETA: 7s - loss: 1.1210 - accuracy: 0.42 - ETA: 7s - loss: 1.0977 - accuracy: 0.42 - ETA: 6s - loss: 1.0898 - accuracy: 0.42 - ETA: 6s - loss: 1.0829 - accuracy: 0.42 - ETA: 6s - loss: 1.0816 - accuracy: 0.42 - ETA: 6s - loss: 1.0798 - accuracy: 0.42 - ETA: 6s - loss: 1.0741 - accuracy: 0.42 - ETA: 6s - loss: 1.0743 - accuracy: 0.42 - ETA: 6s - loss: 1.0697 - accuracy: 0.42 - ETA: 6s - loss: 1.0692 - accuracy: 0.42 - ETA: 6s - loss: 1.0685 - accuracy: 0.42 - ETA: 6s - loss: 1.0648 - accuracy: 0.42 - ETA: 6s - loss: 1.0633 - accuracy: 0.42 - ETA: 6s - loss: 1.0631 - accuracy: 0.42 - ETA: 6s - loss: 1.0623 - accuracy: 0.42 - ETA: 6s - loss: 1.0607 - accuracy: 0.42 - ETA: 6s - loss: 1.0614 - accuracy: 0.42 - ETA: 6s - loss: 1.0616 - accuracy: 0.42 - ETA: 6s - loss: 1.0617 - accuracy: 0.42 - ETA: 6s - loss: 1.0630 - accuracy: 0.42 - ETA: 6s - loss: 1.0629 - accuracy: 0.42 - ETA: 6s - loss: 1.0612 - accuracy: 0.42 - ETA: 6s - loss: 1.0600 - accuracy: 0.42 - ETA: 6s - loss: 1.0606 - accuracy: 0.42 - ETA: 6s - loss: 1.0612 - accuracy: 0.42 - ETA: 5s - loss: 1.0605 - accuracy: 0.42 - ETA: 5s - loss: 1.0593 - accuracy: 0.42 - ETA: 5s - loss: 1.0587 - accuracy: 0.42 - ETA: 5s - loss: 1.0606 - accuracy: 0.42 - ETA: 5s - loss: 1.0598 - accuracy: 0.42 - ETA: 5s - loss: 1.0592 - accuracy: 0.42 - ETA: 5s - loss: 1.0584 - accuracy: 0.42 - ETA: 5s - loss: 1.0581 - accuracy: 0.42 - ETA: 5s - loss: 1.0580 - accuracy: 0.42 - ETA: 5s - loss: 1.0581 - accuracy: 0.42 - ETA: 5s - loss: 1.0576 - accuracy: 0.42 - ETA: 5s - loss: 1.0575 - accuracy: 0.42 - ETA: 5s - loss: 1.0579 - accuracy: 0.42 - ETA: 5s - loss: 1.0585 - accuracy: 0.42 - ETA: 5s - loss: 1.0591 - accuracy: 0.42 - ETA: 5s - loss: 1.0594 - accuracy: 0.42 - ETA: 5s - loss: 1.0592 - accuracy: 0.42 - ETA: 5s - loss: 1.0594 - accuracy: 0.42 - ETA: 5s - loss: 1.0592 - accuracy: 0.42 - ETA: 5s - loss: 1.0591 - accuracy: 0.42 - ETA: 4s - loss: 1.0581 - accuracy: 0.42 - ETA: 4s - loss: 1.0575 - accuracy: 0.42 - ETA: 4s - loss: 1.0577 - accuracy: 0.42 - ETA: 4s - loss: 1.0572 - accuracy: 0.42 - ETA: 4s - loss: 1.0571 - accuracy: 0.42 - ETA: 4s - loss: 1.0571 - accuracy: 0.42 - ETA: 4s - loss: 1.0566 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0555 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0554 - accuracy: 0.42 - ETA: 4s - loss: 1.0556 - accuracy: 0.42 - ETA: 4s - loss: 1.0556 - accuracy: 0.42 - ETA: 4s - loss: 1.0558 - accuracy: 0.42 - ETA: 4s - loss: 1.0558 - accuracy: 0.42 - ETA: 4s - loss: 1.0559 - accuracy: 0.42 - ETA: 4s - loss: 1.0554 - accuracy: 0.42 - ETA: 4s - loss: 1.0551 - accuracy: 0.42 - ETA: 4s - loss: 1.0551 - accuracy: 0.42 - ETA: 4s - loss: 1.0552 - accuracy: 0.42 - ETA: 4s - loss: 1.0552 - accuracy: 0.42 - ETA: 4s - loss: 1.0553 - accuracy: 0.42 - ETA: 4s - loss: 1.0552 - accuracy: 0.42 - ETA: 3s - loss: 1.0551 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0548 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0553 - accuracy: 0.42 - ETA: 3s - loss: 1.0550 - accuracy: 0.42 - ETA: 3s - loss: 1.0550 - accuracy: 0.42 - ETA: 3s - loss: 1.0552 - accuracy: 0.42 - ETA: 3s - loss: 1.0551 - accuracy: 0.42 - ETA: 3s - loss: 1.0551 - accuracy: 0.42 - ETA: 3s - loss: 1.0551 - accuracy: 0.42 - ETA: 3s - loss: 1.0548 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0552 - accuracy: 0.42 - ETA: 3s - loss: 1.0552 - accuracy: 0.42 - ETA: 3s - loss: 1.0551 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0550 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0546 - accuracy: 0.42 - ETA: 3s - loss: 1.0545 - accuracy: 0.42 - ETA: 2s - loss: 1.0547 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0545 - accuracy: 0.42 - ETA: 2s - loss: 1.0543 - accuracy: 0.42 - ETA: 2s - loss: 1.0541 - accuracy: 0.42 - ETA: 2s - loss: 1.0541 - accuracy: 0.42 - ETA: 2s - loss: 1.0539 - accuracy: 0.42 - ETA: 2s - loss: 1.0541 - accuracy: 0.42 - ETA: 2s - loss: 1.0543 - accuracy: 0.42 - ETA: 2s - loss: 1.0542 - accuracy: 0.42 - ETA: 2s - loss: 1.0541 - accuracy: 0.42 - ETA: 2s - loss: 1.0542 - accuracy: 0.42 - ETA: 2s - loss: 1.0540 - accuracy: 0.42 - ETA: 2s - loss: 1.0536 - accuracy: 0.42 - ETA: 2s - loss: 1.0537 - accuracy: 0.42 - ETA: 2s - loss: 1.0539 - accuracy: 0.42 - ETA: 2s - loss: 1.0539 - accuracy: 0.42 - ETA: 2s - loss: 1.0536 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0538 - accuracy: 0.42 - ETA: 1s - loss: 1.0538 - accuracy: 0.42 - ETA: 1s - loss: 1.0540 - accuracy: 0.42 - ETA: 1s - loss: 1.0537 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0533 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 0s - loss: 1.0534 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0540 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0540 - accuracy: 0.42 - ETA: 0s - loss: 1.0540 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0539 - accuracy: 0.42 - ETA: 0s - loss: 1.0537 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0537 - accuracy: 0.42 - ETA: 0s - loss: 1.0537 - accuracy: 0.42 - ETA: 0s - loss: 1.0540 - accuracy: 0.42 - ETA: 0s - loss: 1.0540 - accuracy: 0.42 - ETA: 0s - loss: 1.0542 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - 10s 1ms/step - loss: 1.0538 - accuracy: 0.4257 - val_loss: 1.1490 - val_accuracy: 0.4414\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0387 - accuracy: 0.53 - ETA: 7s - loss: 1.0450 - accuracy: 0.43 - ETA: 7s - loss: 1.0407 - accuracy: 0.43 - ETA: 7s - loss: 1.0436 - accuracy: 0.42 - ETA: 8s - loss: 1.0469 - accuracy: 0.42 - ETA: 8s - loss: 1.0503 - accuracy: 0.42 - ETA: 8s - loss: 1.0488 - accuracy: 0.42 - ETA: 8s - loss: 1.0530 - accuracy: 0.42 - ETA: 7s - loss: 1.0532 - accuracy: 0.42 - ETA: 7s - loss: 1.0521 - accuracy: 0.42 - ETA: 7s - loss: 1.0509 - accuracy: 0.42 - ETA: 7s - loss: 1.0502 - accuracy: 0.42 - ETA: 7s - loss: 1.0501 - accuracy: 0.42 - ETA: 7s - loss: 1.0505 - accuracy: 0.42 - ETA: 7s - loss: 1.0504 - accuracy: 0.42 - ETA: 7s - loss: 1.0525 - accuracy: 0.42 - ETA: 6s - loss: 1.0517 - accuracy: 0.42 - ETA: 7s - loss: 1.0512 - accuracy: 0.42 - ETA: 7s - loss: 1.0500 - accuracy: 0.42 - ETA: 7s - loss: 1.0507 - accuracy: 0.42 - ETA: 7s - loss: 1.0508 - accuracy: 0.42 - ETA: 7s - loss: 1.0511 - accuracy: 0.42 - ETA: 7s - loss: 1.0519 - accuracy: 0.42 - ETA: 7s - loss: 1.0546 - accuracy: 0.42 - ETA: 6s - loss: 1.0538 - accuracy: 0.42 - ETA: 6s - loss: 1.0530 - accuracy: 0.42 - ETA: 6s - loss: 1.0518 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0540 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0542 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0544 - accuracy: 0.42 - ETA: 6s - loss: 1.0563 - accuracy: 0.42 - ETA: 6s - loss: 1.0557 - accuracy: 0.42 - ETA: 6s - loss: 1.0556 - accuracy: 0.42 - ETA: 5s - loss: 1.0557 - accuracy: 0.42 - ETA: 5s - loss: 1.0561 - accuracy: 0.42 - ETA: 5s - loss: 1.0558 - accuracy: 0.42 - ETA: 5s - loss: 1.0557 - accuracy: 0.42 - ETA: 5s - loss: 1.0554 - accuracy: 0.42 - ETA: 5s - loss: 1.0548 - accuracy: 0.42 - ETA: 5s - loss: 1.0544 - accuracy: 0.42 - ETA: 5s - loss: 1.0541 - accuracy: 0.42 - ETA: 5s - loss: 1.0535 - accuracy: 0.42 - ETA: 5s - loss: 1.0535 - accuracy: 0.42 - ETA: 5s - loss: 1.0530 - accuracy: 0.42 - ETA: 5s - loss: 1.0534 - accuracy: 0.42 - ETA: 5s - loss: 1.0528 - accuracy: 0.42 - ETA: 5s - loss: 1.0536 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0537 - accuracy: 0.42 - ETA: 4s - loss: 1.0547 - accuracy: 0.42 - ETA: 4s - loss: 1.0548 - accuracy: 0.42 - ETA: 4s - loss: 1.0549 - accuracy: 0.42 - ETA: 4s - loss: 1.0546 - accuracy: 0.42 - ETA: 4s - loss: 1.0545 - accuracy: 0.42 - ETA: 4s - loss: 1.0544 - accuracy: 0.42 - ETA: 4s - loss: 1.0545 - accuracy: 0.42 - ETA: 4s - loss: 1.0544 - accuracy: 0.42 - ETA: 4s - loss: 1.0542 - accuracy: 0.42 - ETA: 4s - loss: 1.0540 - accuracy: 0.42 - ETA: 4s - loss: 1.0539 - accuracy: 0.42 - ETA: 4s - loss: 1.0541 - accuracy: 0.42 - ETA: 4s - loss: 1.0543 - accuracy: 0.42 - ETA: 4s - loss: 1.0542 - accuracy: 0.42 - ETA: 4s - loss: 1.0543 - accuracy: 0.42 - ETA: 4s - loss: 1.0545 - accuracy: 0.42 - ETA: 4s - loss: 1.0551 - accuracy: 0.42 - ETA: 4s - loss: 1.0549 - accuracy: 0.42 - ETA: 4s - loss: 1.0546 - accuracy: 0.42 - ETA: 4s - loss: 1.0544 - accuracy: 0.42 - ETA: 3s - loss: 1.0545 - accuracy: 0.42 - ETA: 3s - loss: 1.0545 - accuracy: 0.42 - ETA: 3s - loss: 1.0546 - accuracy: 0.42 - ETA: 3s - loss: 1.0543 - accuracy: 0.42 - ETA: 3s - loss: 1.0540 - accuracy: 0.42 - ETA: 3s - loss: 1.0539 - accuracy: 0.42 - ETA: 3s - loss: 1.0536 - accuracy: 0.42 - ETA: 3s - loss: 1.0535 - accuracy: 0.42 - ETA: 3s - loss: 1.0536 - accuracy: 0.42 - ETA: 3s - loss: 1.0533 - accuracy: 0.42 - ETA: 3s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0531 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0522 - accuracy: 0.42 - ETA: 3s - loss: 1.0524 - accuracy: 0.42 - ETA: 3s - loss: 1.0521 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0519 - accuracy: 0.42 - ETA: 3s - loss: 1.0518 - accuracy: 0.42 - ETA: 3s - loss: 1.0518 - accuracy: 0.42 - ETA: 3s - loss: 1.0518 - accuracy: 0.42 - ETA: 3s - loss: 1.0518 - accuracy: 0.42 - ETA: 2s - loss: 1.0520 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0526 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0525 - accuracy: 0.42 - ETA: 2s - loss: 1.0525 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0525 - accuracy: 0.42 - ETA: 2s - loss: 1.0526 - accuracy: 0.42 - ETA: 2s - loss: 1.0528 - accuracy: 0.42 - ETA: 2s - loss: 1.0528 - accuracy: 0.42 - ETA: 2s - loss: 1.0526 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0526 - accuracy: 0.42 - ETA: 2s - loss: 1.0526 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0525 - accuracy: 0.42 - ETA: 2s - loss: 1.0527 - accuracy: 0.42 - ETA: 2s - loss: 1.0528 - accuracy: 0.42 - ETA: 2s - loss: 1.0526 - accuracy: 0.42 - ETA: 2s - loss: 1.0525 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0523 - accuracy: 0.42 - ETA: 1s - loss: 1.0523 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0518 - accuracy: 0.42 - ETA: 1s - loss: 1.0517 - accuracy: 0.42 - ETA: 1s - loss: 1.0518 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0518 - accuracy: 0.42 - ETA: 1s - loss: 1.0518 - accuracy: 0.42 - ETA: 1s - loss: 1.0517 - accuracy: 0.42 - ETA: 1s - loss: 1.0516 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0518 - accuracy: 0.42 - ETA: 1s - loss: 1.0518 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0520 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0519 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0525 - accuracy: 0.42 - ETA: 0s - loss: 1.0526 - accuracy: 0.42 - ETA: 0s - loss: 1.0525 - accuracy: 0.42 - ETA: 0s - loss: 1.0525 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0526 - accuracy: 0.42 - ETA: 0s - loss: 1.0529 - accuracy: 0.42 - ETA: 0s - loss: 1.0528 - accuracy: 0.42 - 10s 1ms/step - loss: 1.0527 - accuracy: 0.4272 - val_loss: 1.0178 - val_accuracy: 0.4105\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 3s - loss: 0.9977 - accuracy: 0.40 - ETA: 7s - loss: 1.0593 - accuracy: 0.41 - ETA: 7s - loss: 1.0632 - accuracy: 0.42 - ETA: 7s - loss: 1.0551 - accuracy: 0.43 - ETA: 7s - loss: 1.0567 - accuracy: 0.43 - ETA: 8s - loss: 1.0560 - accuracy: 0.43 - ETA: 8s - loss: 1.0555 - accuracy: 0.43 - ETA: 8s - loss: 1.0586 - accuracy: 0.43 - ETA: 8s - loss: 1.0565 - accuracy: 0.43 - ETA: 8s - loss: 1.0573 - accuracy: 0.42 - ETA: 8s - loss: 1.0576 - accuracy: 0.42 - ETA: 8s - loss: 1.0564 - accuracy: 0.42 - ETA: 8s - loss: 1.0536 - accuracy: 0.43 - ETA: 8s - loss: 1.0523 - accuracy: 0.43 - ETA: 7s - loss: 1.0513 - accuracy: 0.42 - ETA: 7s - loss: 1.0512 - accuracy: 0.43 - ETA: 7s - loss: 1.0514 - accuracy: 0.43 - ETA: 7s - loss: 1.0532 - accuracy: 0.42 - ETA: 7s - loss: 1.0517 - accuracy: 0.42 - ETA: 7s - loss: 1.0515 - accuracy: 0.42 - ETA: 7s - loss: 1.0519 - accuracy: 0.42 - ETA: 7s - loss: 1.0513 - accuracy: 0.42 - ETA: 7s - loss: 1.0526 - accuracy: 0.42 - ETA: 7s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0566 - accuracy: 0.42 - ETA: 6s - loss: 1.0558 - accuracy: 0.42 - ETA: 7s - loss: 1.0558 - accuracy: 0.42 - ETA: 7s - loss: 1.0558 - accuracy: 0.42 - ETA: 7s - loss: 1.0556 - accuracy: 0.42 - ETA: 7s - loss: 1.0542 - accuracy: 0.42 - ETA: 6s - loss: 1.0544 - accuracy: 0.42 - ETA: 7s - loss: 1.0542 - accuracy: 0.42 - ETA: 6s - loss: 1.0536 - accuracy: 0.42 - ETA: 6s - loss: 1.0534 - accuracy: 0.42 - ETA: 6s - loss: 1.0534 - accuracy: 0.42 - ETA: 6s - loss: 1.0542 - accuracy: 0.42 - ETA: 6s - loss: 1.0539 - accuracy: 0.42 - ETA: 6s - loss: 1.0540 - accuracy: 0.42 - ETA: 6s - loss: 1.0541 - accuracy: 0.42 - ETA: 6s - loss: 1.0540 - accuracy: 0.42 - ETA: 6s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0536 - accuracy: 0.42 - ETA: 6s - loss: 1.0526 - accuracy: 0.42 - ETA: 6s - loss: 1.0528 - accuracy: 0.42 - ETA: 6s - loss: 1.0530 - accuracy: 0.42 - ETA: 6s - loss: 1.0534 - accuracy: 0.42 - ETA: 6s - loss: 1.0533 - accuracy: 0.42 - ETA: 6s - loss: 1.0531 - accuracy: 0.42 - ETA: 6s - loss: 1.0532 - accuracy: 0.42 - ETA: 6s - loss: 1.0529 - accuracy: 0.42 - ETA: 6s - loss: 1.0526 - accuracy: 0.42 - ETA: 6s - loss: 1.0527 - accuracy: 0.42 - ETA: 7s - loss: 1.0526 - accuracy: 0.42 - ETA: 7s - loss: 1.0527 - accuracy: 0.42 - ETA: 7s - loss: 1.0528 - accuracy: 0.42 - ETA: 7s - loss: 1.0526 - accuracy: 0.42 - ETA: 7s - loss: 1.0528 - accuracy: 0.42 - ETA: 6s - loss: 1.0531 - accuracy: 0.42 - ETA: 6s - loss: 1.0529 - accuracy: 0.42 - ETA: 6s - loss: 1.0527 - accuracy: 0.42 - ETA: 6s - loss: 1.0525 - accuracy: 0.42 - ETA: 6s - loss: 1.0524 - accuracy: 0.42 - ETA: 7s - loss: 1.0522 - accuracy: 0.42 - ETA: 6s - loss: 1.0524 - accuracy: 0.42 - ETA: 6s - loss: 1.0524 - accuracy: 0.42 - ETA: 6s - loss: 1.0522 - accuracy: 0.42 - ETA: 6s - loss: 1.0521 - accuracy: 0.42 - ETA: 6s - loss: 1.0523 - accuracy: 0.42 - ETA: 6s - loss: 1.0523 - accuracy: 0.42 - ETA: 6s - loss: 1.0527 - accuracy: 0.42 - ETA: 6s - loss: 1.0523 - accuracy: 0.42 - ETA: 6s - loss: 1.0522 - accuracy: 0.42 - ETA: 6s - loss: 1.0521 - accuracy: 0.42 - ETA: 6s - loss: 1.0519 - accuracy: 0.42 - ETA: 6s - loss: 1.0521 - accuracy: 0.42 - ETA: 6s - loss: 1.0520 - accuracy: 0.42 - ETA: 5s - loss: 1.0518 - accuracy: 0.42 - ETA: 6s - loss: 1.0518 - accuracy: 0.42 - ETA: 5s - loss: 1.0519 - accuracy: 0.42 - ETA: 5s - loss: 1.0516 - accuracy: 0.42 - ETA: 5s - loss: 1.0515 - accuracy: 0.42 - ETA: 5s - loss: 1.0512 - accuracy: 0.42 - ETA: 5s - loss: 1.0512 - accuracy: 0.42 - ETA: 5s - loss: 1.0511 - accuracy: 0.42 - ETA: 5s - loss: 1.0512 - accuracy: 0.42 - ETA: 5s - loss: 1.0510 - accuracy: 0.42 - ETA: 5s - loss: 1.0509 - accuracy: 0.42 - ETA: 5s - loss: 1.0512 - accuracy: 0.42 - ETA: 5s - loss: 1.0509 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0508 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0505 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0505 - accuracy: 0.42 - ETA: 4s - loss: 1.0504 - accuracy: 0.42 - ETA: 4s - loss: 1.0508 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0505 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0502 - accuracy: 0.42 - ETA: 3s - loss: 1.0503 - accuracy: 0.42 - ETA: 3s - loss: 1.0501 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0498 - accuracy: 0.42 - ETA: 3s - loss: 1.0500 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0503 - accuracy: 0.42 - ETA: 3s - loss: 1.0501 - accuracy: 0.42 - ETA: 3s - loss: 1.0500 - accuracy: 0.42 - ETA: 3s - loss: 1.0500 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0505 - accuracy: 0.42 - ETA: 2s - loss: 1.0506 - accuracy: 0.42 - ETA: 2s - loss: 1.0505 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0512 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0514 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - 10s 1ms/step - loss: 1.0513 - accuracy: 0.4271 - val_loss: 1.1038 - val_accuracy: 0.4414\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 2s - loss: 1.2281 - accuracy: 0.28 - ETA: 9s - loss: 1.0187 - accuracy: 0.44 - ETA: 10s - loss: 1.0306 - accuracy: 0.425 - ETA: 10s - loss: 1.0330 - accuracy: 0.421 - ETA: 9s - loss: 1.0403 - accuracy: 0.421 - ETA: 8s - loss: 1.0526 - accuracy: 0.42 - ETA: 8s - loss: 1.0512 - accuracy: 0.42 - ETA: 8s - loss: 1.0564 - accuracy: 0.42 - ETA: 8s - loss: 1.0541 - accuracy: 0.42 - ETA: 9s - loss: 1.0543 - accuracy: 0.42 - ETA: 9s - loss: 1.0555 - accuracy: 0.42 - ETA: 9s - loss: 1.0567 - accuracy: 0.42 - ETA: 9s - loss: 1.0543 - accuracy: 0.42 - ETA: 9s - loss: 1.0534 - accuracy: 0.42 - ETA: 8s - loss: 1.0519 - accuracy: 0.42 - ETA: 8s - loss: 1.0508 - accuracy: 0.42 - ETA: 8s - loss: 1.0511 - accuracy: 0.42 - ETA: 8s - loss: 1.0505 - accuracy: 0.42 - ETA: 7s - loss: 1.0544 - accuracy: 0.42 - ETA: 7s - loss: 1.0547 - accuracy: 0.42 - ETA: 7s - loss: 1.0555 - accuracy: 0.42 - ETA: 7s - loss: 1.0567 - accuracy: 0.42 - ETA: 7s - loss: 1.0559 - accuracy: 0.42 - ETA: 7s - loss: 1.0542 - accuracy: 0.42 - ETA: 7s - loss: 1.0551 - accuracy: 0.42 - ETA: 7s - loss: 1.0545 - accuracy: 0.42 - ETA: 7s - loss: 1.0551 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0551 - accuracy: 0.42 - ETA: 6s - loss: 1.0551 - accuracy: 0.42 - ETA: 6s - loss: 1.0552 - accuracy: 0.42 - ETA: 6s - loss: 1.0552 - accuracy: 0.42 - ETA: 6s - loss: 1.0547 - accuracy: 0.42 - ETA: 6s - loss: 1.0552 - accuracy: 0.42 - ETA: 6s - loss: 1.0552 - accuracy: 0.42 - ETA: 6s - loss: 1.0549 - accuracy: 0.42 - ETA: 6s - loss: 1.0541 - accuracy: 0.42 - ETA: 7s - loss: 1.0543 - accuracy: 0.42 - ETA: 7s - loss: 1.0550 - accuracy: 0.42 - ETA: 7s - loss: 1.0554 - accuracy: 0.42 - ETA: 6s - loss: 1.0561 - accuracy: 0.42 - ETA: 6s - loss: 1.0565 - accuracy: 0.42 - ETA: 6s - loss: 1.0559 - accuracy: 0.42 - ETA: 6s - loss: 1.0559 - accuracy: 0.42 - ETA: 6s - loss: 1.0555 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0546 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0541 - accuracy: 0.42 - ETA: 6s - loss: 1.0546 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0552 - accuracy: 0.42 - ETA: 6s - loss: 1.0547 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0543 - accuracy: 0.42 - ETA: 6s - loss: 1.0541 - accuracy: 0.42 - ETA: 6s - loss: 1.0544 - accuracy: 0.42 - ETA: 5s - loss: 1.0542 - accuracy: 0.42 - ETA: 5s - loss: 1.0543 - accuracy: 0.42 - ETA: 5s - loss: 1.0539 - accuracy: 0.42 - ETA: 5s - loss: 1.0545 - accuracy: 0.42 - ETA: 5s - loss: 1.0540 - accuracy: 0.42 - ETA: 5s - loss: 1.0541 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0537 - accuracy: 0.42 - ETA: 5s - loss: 1.0537 - accuracy: 0.42 - ETA: 5s - loss: 1.0534 - accuracy: 0.42 - ETA: 5s - loss: 1.0536 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0539 - accuracy: 0.42 - ETA: 5s - loss: 1.0536 - accuracy: 0.42 - ETA: 4s - loss: 1.0535 - accuracy: 0.42 - ETA: 4s - loss: 1.0530 - accuracy: 0.42 - ETA: 4s - loss: 1.0537 - accuracy: 0.42 - ETA: 4s - loss: 1.0540 - accuracy: 0.42 - ETA: 4s - loss: 1.0534 - accuracy: 0.42 - ETA: 4s - loss: 1.0533 - accuracy: 0.42 - ETA: 4s - loss: 1.0532 - accuracy: 0.42 - ETA: 4s - loss: 1.0532 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0529 - accuracy: 0.42 - ETA: 4s - loss: 1.0528 - accuracy: 0.42 - ETA: 4s - loss: 1.0529 - accuracy: 0.42 - ETA: 4s - loss: 1.0527 - accuracy: 0.42 - ETA: 4s - loss: 1.0527 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0529 - accuracy: 0.42 - ETA: 4s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0531 - accuracy: 0.42 - ETA: 3s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0538 - accuracy: 0.42 - ETA: 3s - loss: 1.0539 - accuracy: 0.42 - ETA: 3s - loss: 1.0537 - accuracy: 0.42 - ETA: 3s - loss: 1.0536 - accuracy: 0.42 - ETA: 3s - loss: 1.0537 - accuracy: 0.42 - ETA: 3s - loss: 1.0536 - accuracy: 0.42 - ETA: 3s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0531 - accuracy: 0.42 - ETA: 3s - loss: 1.0532 - accuracy: 0.42 - ETA: 3s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0537 - accuracy: 0.42 - ETA: 3s - loss: 1.0535 - accuracy: 0.42 - ETA: 3s - loss: 1.0534 - accuracy: 0.42 - ETA: 3s - loss: 1.0535 - accuracy: 0.42 - ETA: 3s - loss: 1.0535 - accuracy: 0.42 - ETA: 3s - loss: 1.0533 - accuracy: 0.42 - ETA: 2s - loss: 1.0533 - accuracy: 0.42 - ETA: 2s - loss: 1.0532 - accuracy: 0.42 - ETA: 2s - loss: 1.0536 - accuracy: 0.42 - ETA: 2s - loss: 1.0534 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0534 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0536 - accuracy: 0.42 - ETA: 2s - loss: 1.0537 - accuracy: 0.42 - ETA: 2s - loss: 1.0536 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0533 - accuracy: 0.42 - ETA: 2s - loss: 1.0534 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0537 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0533 - accuracy: 0.42 - ETA: 1s - loss: 1.0532 - accuracy: 0.42 - ETA: 1s - loss: 1.0531 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0537 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0539 - accuracy: 0.42 - ETA: 1s - loss: 1.0539 - accuracy: 0.42 - ETA: 1s - loss: 1.0542 - accuracy: 0.42 - ETA: 1s - loss: 1.0541 - accuracy: 0.42 - ETA: 1s - loss: 1.0541 - accuracy: 0.42 - ETA: 1s - loss: 1.0540 - accuracy: 0.42 - ETA: 1s - loss: 1.0538 - accuracy: 0.42 - ETA: 1s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0536 - accuracy: 0.42 - ETA: 0s - loss: 1.0536 - accuracy: 0.42 - ETA: 0s - loss: 1.0536 - accuracy: 0.42 - ETA: 0s - loss: 1.0536 - accuracy: 0.42 - ETA: 0s - loss: 1.0535 - accuracy: 0.42 - ETA: 0s - loss: 1.0533 - accuracy: 0.42 - ETA: 0s - loss: 1.0533 - accuracy: 0.42 - ETA: 0s - loss: 1.0532 - accuracy: 0.42 - ETA: 0s - loss: 1.0532 - accuracy: 0.42 - ETA: 0s - loss: 1.0532 - accuracy: 0.42 - ETA: 0s - loss: 1.0536 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0538 - accuracy: 0.42 - ETA: 0s - loss: 1.0537 - accuracy: 0.42 - ETA: 0s - loss: 1.0536 - accuracy: 0.42 - ETA: 0s - loss: 1.0537 - accuracy: 0.42 - 9s 1ms/step - loss: 1.0537 - accuracy: 0.4254 - val_loss: 1.0471 - val_accuracy: 0.4414\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 0.9477 - accuracy: 0.40 - ETA: 6s - loss: 1.0405 - accuracy: 0.43 - ETA: 6s - loss: 1.0513 - accuracy: 0.42 - ETA: 7s - loss: 1.0538 - accuracy: 0.42 - ETA: 6s - loss: 1.0525 - accuracy: 0.42 - ETA: 6s - loss: 1.0549 - accuracy: 0.43 - ETA: 6s - loss: 1.0594 - accuracy: 0.42 - ETA: 7s - loss: 1.0593 - accuracy: 0.42 - ETA: 8s - loss: 1.0583 - accuracy: 0.42 - ETA: 8s - loss: 1.0573 - accuracy: 0.42 - ETA: 8s - loss: 1.0580 - accuracy: 0.42 - ETA: 8s - loss: 1.0609 - accuracy: 0.42 - ETA: 8s - loss: 1.0604 - accuracy: 0.42 - ETA: 7s - loss: 1.0611 - accuracy: 0.42 - ETA: 7s - loss: 1.0613 - accuracy: 0.42 - ETA: 7s - loss: 1.0607 - accuracy: 0.42 - ETA: 7s - loss: 1.0601 - accuracy: 0.42 - ETA: 7s - loss: 1.0583 - accuracy: 0.42 - ETA: 7s - loss: 1.0561 - accuracy: 0.42 - ETA: 7s - loss: 1.0554 - accuracy: 0.42 - ETA: 7s - loss: 1.0547 - accuracy: 0.42 - ETA: 7s - loss: 1.0549 - accuracy: 0.42 - ETA: 7s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0534 - accuracy: 0.42 - ETA: 6s - loss: 1.0540 - accuracy: 0.42 - ETA: 6s - loss: 1.0547 - accuracy: 0.42 - ETA: 6s - loss: 1.0556 - accuracy: 0.42 - ETA: 6s - loss: 1.0556 - accuracy: 0.42 - ETA: 6s - loss: 1.0549 - accuracy: 0.42 - ETA: 6s - loss: 1.0544 - accuracy: 0.42 - ETA: 6s - loss: 1.0538 - accuracy: 0.42 - ETA: 6s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0535 - accuracy: 0.42 - ETA: 6s - loss: 1.0548 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0547 - accuracy: 0.42 - ETA: 6s - loss: 1.0546 - accuracy: 0.42 - ETA: 6s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0541 - accuracy: 0.42 - ETA: 6s - loss: 1.0550 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0543 - accuracy: 0.42 - ETA: 6s - loss: 1.0553 - accuracy: 0.42 - ETA: 6s - loss: 1.0547 - accuracy: 0.42 - ETA: 5s - loss: 1.0546 - accuracy: 0.42 - ETA: 5s - loss: 1.0543 - accuracy: 0.42 - ETA: 5s - loss: 1.0550 - accuracy: 0.42 - ETA: 5s - loss: 1.0552 - accuracy: 0.42 - ETA: 5s - loss: 1.0546 - accuracy: 0.42 - ETA: 5s - loss: 1.0552 - accuracy: 0.42 - ETA: 5s - loss: 1.0547 - accuracy: 0.42 - ETA: 5s - loss: 1.0543 - accuracy: 0.42 - ETA: 5s - loss: 1.0539 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0534 - accuracy: 0.42 - ETA: 5s - loss: 1.0537 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0542 - accuracy: 0.42 - ETA: 5s - loss: 1.0549 - accuracy: 0.42 - ETA: 5s - loss: 1.0554 - accuracy: 0.42 - ETA: 5s - loss: 1.0556 - accuracy: 0.42 - ETA: 4s - loss: 1.0559 - accuracy: 0.42 - ETA: 4s - loss: 1.0559 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0559 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0555 - accuracy: 0.42 - ETA: 4s - loss: 1.0553 - accuracy: 0.42 - ETA: 4s - loss: 1.0555 - accuracy: 0.42 - ETA: 4s - loss: 1.0556 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0560 - accuracy: 0.42 - ETA: 4s - loss: 1.0560 - accuracy: 0.42 - ETA: 4s - loss: 1.0560 - accuracy: 0.42 - ETA: 4s - loss: 1.0558 - accuracy: 0.42 - ETA: 4s - loss: 1.0556 - accuracy: 0.42 - ETA: 4s - loss: 1.0558 - accuracy: 0.42 - ETA: 4s - loss: 1.0554 - accuracy: 0.42 - ETA: 4s - loss: 1.0555 - accuracy: 0.42 - ETA: 4s - loss: 1.0555 - accuracy: 0.42 - ETA: 4s - loss: 1.0559 - accuracy: 0.42 - ETA: 4s - loss: 1.0561 - accuracy: 0.42 - ETA: 4s - loss: 1.0561 - accuracy: 0.42 - ETA: 4s - loss: 1.0564 - accuracy: 0.42 - ETA: 4s - loss: 1.0571 - accuracy: 0.42 - ETA: 4s - loss: 1.0568 - accuracy: 0.42 - ETA: 4s - loss: 1.0565 - accuracy: 0.42 - ETA: 4s - loss: 1.0563 - accuracy: 0.42 - ETA: 4s - loss: 1.0562 - accuracy: 0.42 - ETA: 4s - loss: 1.0562 - accuracy: 0.42 - ETA: 4s - loss: 1.0562 - accuracy: 0.42 - ETA: 4s - loss: 1.0559 - accuracy: 0.42 - ETA: 4s - loss: 1.0557 - accuracy: 0.42 - ETA: 4s - loss: 1.0556 - accuracy: 0.42 - ETA: 3s - loss: 1.0553 - accuracy: 0.42 - ETA: 3s - loss: 1.0552 - accuracy: 0.42 - ETA: 3s - loss: 1.0551 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0548 - accuracy: 0.42 - ETA: 3s - loss: 1.0550 - accuracy: 0.42 - ETA: 3s - loss: 1.0549 - accuracy: 0.42 - ETA: 3s - loss: 1.0548 - accuracy: 0.42 - ETA: 3s - loss: 1.0546 - accuracy: 0.42 - ETA: 3s - loss: 1.0547 - accuracy: 0.42 - ETA: 3s - loss: 1.0544 - accuracy: 0.42 - ETA: 3s - loss: 1.0541 - accuracy: 0.42 - ETA: 3s - loss: 1.0540 - accuracy: 0.42 - ETA: 3s - loss: 1.0543 - accuracy: 0.42 - ETA: 3s - loss: 1.0546 - accuracy: 0.42 - ETA: 3s - loss: 1.0547 - accuracy: 0.42 - ETA: 3s - loss: 1.0546 - accuracy: 0.42 - ETA: 3s - loss: 1.0547 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0547 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0543 - accuracy: 0.42 - ETA: 2s - loss: 1.0544 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0545 - accuracy: 0.42 - ETA: 2s - loss: 1.0547 - accuracy: 0.42 - ETA: 2s - loss: 1.0549 - accuracy: 0.42 - ETA: 2s - loss: 1.0548 - accuracy: 0.42 - ETA: 2s - loss: 1.0549 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0547 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 2s - loss: 1.0546 - accuracy: 0.42 - ETA: 1s - loss: 1.0549 - accuracy: 0.42 - ETA: 1s - loss: 1.0548 - accuracy: 0.42 - ETA: 1s - loss: 1.0548 - accuracy: 0.42 - ETA: 1s - loss: 1.0547 - accuracy: 0.42 - ETA: 1s - loss: 1.0546 - accuracy: 0.42 - ETA: 1s - loss: 1.0543 - accuracy: 0.42 - ETA: 1s - loss: 1.0541 - accuracy: 0.42 - ETA: 1s - loss: 1.0540 - accuracy: 0.42 - ETA: 1s - loss: 1.0539 - accuracy: 0.42 - ETA: 1s - loss: 1.0538 - accuracy: 0.42 - ETA: 1s - loss: 1.0538 - accuracy: 0.42 - ETA: 1s - loss: 1.0537 - accuracy: 0.42 - ETA: 1s - loss: 1.0536 - accuracy: 0.42 - ETA: 1s - loss: 1.0534 - accuracy: 0.42 - ETA: 1s - loss: 1.0533 - accuracy: 0.42 - ETA: 1s - loss: 1.0532 - accuracy: 0.42 - ETA: 0s - loss: 1.0533 - accuracy: 0.42 - ETA: 0s - loss: 1.0532 - accuracy: 0.42 - ETA: 0s - loss: 1.0534 - accuracy: 0.42 - ETA: 0s - loss: 1.0533 - accuracy: 0.42 - ETA: 0s - loss: 1.0532 - accuracy: 0.42 - ETA: 0s - loss: 1.0530 - accuracy: 0.42 - ETA: 0s - loss: 1.0531 - accuracy: 0.42 - ETA: 0s - loss: 1.0531 - accuracy: 0.42 - ETA: 0s - loss: 1.0530 - accuracy: 0.42 - ETA: 0s - loss: 1.0531 - accuracy: 0.42 - ETA: 0s - loss: 1.0530 - accuracy: 0.42 - ETA: 0s - loss: 1.0530 - accuracy: 0.42 - ETA: 0s - loss: 1.0531 - accuracy: 0.42 - ETA: 0s - loss: 1.0533 - accuracy: 0.42 - ETA: 0s - loss: 1.0531 - accuracy: 0.42 - ETA: 0s - loss: 1.0531 - accuracy: 0.42 - 9s 998us/step - loss: 1.0534 - accuracy: 0.4258 - val_loss: 1.1574 - val_accuracy: 0.4414\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0538 - accuracy: 0.43 - ETA: 6s - loss: 1.0467 - accuracy: 0.43 - ETA: 6s - loss: 1.0552 - accuracy: 0.42 - ETA: 6s - loss: 1.0490 - accuracy: 0.42 - ETA: 6s - loss: 1.0563 - accuracy: 0.42 - ETA: 6s - loss: 1.0534 - accuracy: 0.43 - ETA: 6s - loss: 1.0506 - accuracy: 0.43 - ETA: 6s - loss: 1.0488 - accuracy: 0.42 - ETA: 6s - loss: 1.0464 - accuracy: 0.42 - ETA: 6s - loss: 1.0452 - accuracy: 0.42 - ETA: 6s - loss: 1.0446 - accuracy: 0.42 - ETA: 6s - loss: 1.0432 - accuracy: 0.42 - ETA: 6s - loss: 1.0472 - accuracy: 0.42 - ETA: 6s - loss: 1.0477 - accuracy: 0.42 - ETA: 6s - loss: 1.0482 - accuracy: 0.42 - ETA: 6s - loss: 1.0460 - accuracy: 0.42 - ETA: 6s - loss: 1.0492 - accuracy: 0.42 - ETA: 6s - loss: 1.0484 - accuracy: 0.42 - ETA: 6s - loss: 1.0473 - accuracy: 0.42 - ETA: 5s - loss: 1.0471 - accuracy: 0.42 - ETA: 5s - loss: 1.0469 - accuracy: 0.42 - ETA: 5s - loss: 1.0464 - accuracy: 0.42 - ETA: 5s - loss: 1.0451 - accuracy: 0.42 - ETA: 5s - loss: 1.0452 - accuracy: 0.42 - ETA: 5s - loss: 1.0451 - accuracy: 0.42 - ETA: 5s - loss: 1.0449 - accuracy: 0.42 - ETA: 5s - loss: 1.0460 - accuracy: 0.42 - ETA: 5s - loss: 1.0463 - accuracy: 0.42 - ETA: 5s - loss: 1.0466 - accuracy: 0.42 - ETA: 5s - loss: 1.0475 - accuracy: 0.42 - ETA: 5s - loss: 1.0473 - accuracy: 0.42 - ETA: 5s - loss: 1.0468 - accuracy: 0.42 - ETA: 5s - loss: 1.0456 - accuracy: 0.42 - ETA: 5s - loss: 1.0457 - accuracy: 0.42 - ETA: 5s - loss: 1.0454 - accuracy: 0.42 - ETA: 5s - loss: 1.0465 - accuracy: 0.42 - ETA: 5s - loss: 1.0468 - accuracy: 0.42 - ETA: 4s - loss: 1.0469 - accuracy: 0.42 - ETA: 4s - loss: 1.0476 - accuracy: 0.42 - ETA: 4s - loss: 1.0472 - accuracy: 0.42 - ETA: 4s - loss: 1.0474 - accuracy: 0.42 - ETA: 4s - loss: 1.0468 - accuracy: 0.42 - ETA: 4s - loss: 1.0470 - accuracy: 0.42 - ETA: 4s - loss: 1.0468 - accuracy: 0.42 - ETA: 4s - loss: 1.0465 - accuracy: 0.42 - ETA: 4s - loss: 1.0465 - accuracy: 0.42 - ETA: 4s - loss: 1.0468 - accuracy: 0.42 - ETA: 4s - loss: 1.0467 - accuracy: 0.42 - ETA: 4s - loss: 1.0470 - accuracy: 0.42 - ETA: 4s - loss: 1.0476 - accuracy: 0.42 - ETA: 4s - loss: 1.0474 - accuracy: 0.42 - ETA: 4s - loss: 1.0481 - accuracy: 0.42 - ETA: 4s - loss: 1.0490 - accuracy: 0.42 - ETA: 4s - loss: 1.0490 - accuracy: 0.42 - ETA: 4s - loss: 1.0491 - accuracy: 0.42 - ETA: 4s - loss: 1.0487 - accuracy: 0.42 - ETA: 4s - loss: 1.0490 - accuracy: 0.42 - ETA: 3s - loss: 1.0491 - accuracy: 0.42 - ETA: 3s - loss: 1.0493 - accuracy: 0.42 - ETA: 3s - loss: 1.0493 - accuracy: 0.42 - ETA: 3s - loss: 1.0492 - accuracy: 0.42 - ETA: 3s - loss: 1.0496 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0503 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0502 - accuracy: 0.42 - ETA: 3s - loss: 1.0501 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0503 - accuracy: 0.42 - ETA: 3s - loss: 1.0506 - accuracy: 0.42 - ETA: 3s - loss: 1.0507 - accuracy: 0.42 - ETA: 3s - loss: 1.0508 - accuracy: 0.42 - ETA: 3s - loss: 1.0511 - accuracy: 0.42 - ETA: 3s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0506 - accuracy: 0.42 - ETA: 2s - loss: 1.0506 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0505 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0500 - accuracy: 0.42 - ETA: 2s - loss: 1.0499 - accuracy: 0.42 - ETA: 2s - loss: 1.0497 - accuracy: 0.42 - ETA: 2s - loss: 1.0496 - accuracy: 0.42 - ETA: 2s - loss: 1.0495 - accuracy: 0.42 - ETA: 2s - loss: 1.0497 - accuracy: 0.42 - ETA: 2s - loss: 1.0498 - accuracy: 0.42 - ETA: 2s - loss: 1.0499 - accuracy: 0.42 - ETA: 2s - loss: 1.0499 - accuracy: 0.42 - ETA: 2s - loss: 1.0499 - accuracy: 0.42 - ETA: 2s - loss: 1.0501 - accuracy: 0.42 - ETA: 2s - loss: 1.0502 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0518 - accuracy: 0.42 - ETA: 2s - loss: 1.0516 - accuracy: 0.42 - ETA: 1s - loss: 1.0515 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 1s - loss: 1.0512 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 1s - loss: 1.0512 - accuracy: 0.42 - ETA: 1s - loss: 1.0511 - accuracy: 0.42 - ETA: 1s - loss: 1.0511 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0515 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0516 - accuracy: 0.42 - ETA: 1s - loss: 1.0517 - accuracy: 0.42 - ETA: 1s - loss: 1.0514 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 1s - loss: 1.0513 - accuracy: 0.42 - ETA: 1s - loss: 1.0515 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0515 - accuracy: 0.42 - ETA: 0s - loss: 1.0515 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0514 - accuracy: 0.42 - ETA: 0s - loss: 1.0514 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - ETA: 0s - loss: 1.0515 - accuracy: 0.42 - ETA: 0s - loss: 1.0516 - accuracy: 0.42 - ETA: 0s - loss: 1.0517 - accuracy: 0.42 - 8s 866us/step - loss: 1.0517 - accuracy: 0.4269 - val_loss: 1.0222 - val_accuracy: 0.4414\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0179 - accuracy: 0.46 - ETA: 7s - loss: 1.0507 - accuracy: 0.43 - ETA: 7s - loss: 1.0526 - accuracy: 0.42 - ETA: 8s - loss: 1.0426 - accuracy: 0.43 - ETA: 8s - loss: 1.0440 - accuracy: 0.43 - ETA: 9s - loss: 1.0434 - accuracy: 0.43 - ETA: 9s - loss: 1.0455 - accuracy: 0.43 - ETA: 10s - loss: 1.0477 - accuracy: 0.430 - ETA: 11s - loss: 1.0485 - accuracy: 0.428 - ETA: 12s - loss: 1.0482 - accuracy: 0.428 - ETA: 12s - loss: 1.0468 - accuracy: 0.428 - ETA: 11s - loss: 1.0508 - accuracy: 0.426 - ETA: 11s - loss: 1.0497 - accuracy: 0.426 - ETA: 10s - loss: 1.0509 - accuracy: 0.426 - ETA: 10s - loss: 1.0512 - accuracy: 0.426 - ETA: 10s - loss: 1.0508 - accuracy: 0.426 - ETA: 9s - loss: 1.0532 - accuracy: 0.427 - ETA: 9s - loss: 1.0526 - accuracy: 0.42 - ETA: 9s - loss: 1.0540 - accuracy: 0.42 - ETA: 9s - loss: 1.0532 - accuracy: 0.42 - ETA: 9s - loss: 1.0527 - accuracy: 0.42 - ETA: 8s - loss: 1.0523 - accuracy: 0.42 - ETA: 8s - loss: 1.0504 - accuracy: 0.42 - ETA: 8s - loss: 1.0502 - accuracy: 0.42 - ETA: 8s - loss: 1.0531 - accuracy: 0.42 - ETA: 7s - loss: 1.0558 - accuracy: 0.42 - ETA: 7s - loss: 1.0548 - accuracy: 0.42 - ETA: 7s - loss: 1.0542 - accuracy: 0.42 - ETA: 7s - loss: 1.0539 - accuracy: 0.42 - ETA: 7s - loss: 1.0537 - accuracy: 0.42 - ETA: 7s - loss: 1.0537 - accuracy: 0.42 - ETA: 7s - loss: 1.0532 - accuracy: 0.42 - ETA: 7s - loss: 1.0525 - accuracy: 0.42 - ETA: 7s - loss: 1.0518 - accuracy: 0.42 - ETA: 7s - loss: 1.0514 - accuracy: 0.42 - ETA: 7s - loss: 1.0514 - accuracy: 0.42 - ETA: 6s - loss: 1.0512 - accuracy: 0.42 - ETA: 6s - loss: 1.0513 - accuracy: 0.42 - ETA: 6s - loss: 1.0517 - accuracy: 0.42 - ETA: 6s - loss: 1.0517 - accuracy: 0.42 - ETA: 6s - loss: 1.0510 - accuracy: 0.42 - ETA: 6s - loss: 1.0506 - accuracy: 0.42 - ETA: 6s - loss: 1.0507 - accuracy: 0.42 - ETA: 6s - loss: 1.0505 - accuracy: 0.42 - ETA: 6s - loss: 1.0500 - accuracy: 0.42 - ETA: 6s - loss: 1.0499 - accuracy: 0.42 - ETA: 6s - loss: 1.0493 - accuracy: 0.42 - ETA: 6s - loss: 1.0493 - accuracy: 0.42 - ETA: 6s - loss: 1.0496 - accuracy: 0.42 - ETA: 6s - loss: 1.0494 - accuracy: 0.42 - ETA: 5s - loss: 1.0492 - accuracy: 0.42 - ETA: 5s - loss: 1.0488 - accuracy: 0.42 - ETA: 5s - loss: 1.0486 - accuracy: 0.42 - ETA: 5s - loss: 1.0488 - accuracy: 0.42 - ETA: 5s - loss: 1.0500 - accuracy: 0.42 - ETA: 5s - loss: 1.0505 - accuracy: 0.42 - ETA: 5s - loss: 1.0502 - accuracy: 0.42 - ETA: 5s - loss: 1.0506 - accuracy: 0.42 - ETA: 5s - loss: 1.0510 - accuracy: 0.42 - ETA: 5s - loss: 1.0511 - accuracy: 0.42 - ETA: 5s - loss: 1.0511 - accuracy: 0.42 - ETA: 5s - loss: 1.0508 - accuracy: 0.42 - ETA: 5s - loss: 1.0506 - accuracy: 0.42 - ETA: 5s - loss: 1.0503 - accuracy: 0.42 - ETA: 5s - loss: 1.0502 - accuracy: 0.42 - ETA: 5s - loss: 1.0505 - accuracy: 0.42 - ETA: 5s - loss: 1.0508 - accuracy: 0.42 - ETA: 5s - loss: 1.0514 - accuracy: 0.42 - ETA: 5s - loss: 1.0510 - accuracy: 0.42 - ETA: 5s - loss: 1.0510 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0504 - accuracy: 0.42 - ETA: 4s - loss: 1.0501 - accuracy: 0.42 - ETA: 4s - loss: 1.0501 - accuracy: 0.42 - ETA: 4s - loss: 1.0498 - accuracy: 0.42 - ETA: 4s - loss: 1.0495 - accuracy: 0.42 - ETA: 4s - loss: 1.0497 - accuracy: 0.42 - ETA: 4s - loss: 1.0499 - accuracy: 0.42 - ETA: 4s - loss: 1.0501 - accuracy: 0.42 - ETA: 3s - loss: 1.0498 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0498 - accuracy: 0.42 - ETA: 3s - loss: 1.0498 - accuracy: 0.42 - ETA: 3s - loss: 1.0501 - accuracy: 0.42 - ETA: 3s - loss: 1.0502 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0506 - accuracy: 0.42 - ETA: 3s - loss: 1.0506 - accuracy: 0.42 - ETA: 3s - loss: 1.0506 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0503 - accuracy: 0.42 - ETA: 3s - loss: 1.0507 - accuracy: 0.42 - ETA: 3s - loss: 1.0505 - accuracy: 0.42 - ETA: 3s - loss: 1.0504 - accuracy: 0.42 - ETA: 3s - loss: 1.0506 - accuracy: 0.42 - ETA: 2s - loss: 1.0514 - accuracy: 0.42 - ETA: 2s - loss: 1.0511 - accuracy: 0.42 - ETA: 2s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0511 - accuracy: 0.42 - ETA: 2s - loss: 1.0511 - accuracy: 0.42 - ETA: 2s - loss: 1.0511 - accuracy: 0.42 - ETA: 2s - loss: 1.0512 - accuracy: 0.42 - ETA: 2s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0510 - accuracy: 0.42 - ETA: 1s - loss: 1.0510 - accuracy: 0.42 - ETA: 1s - loss: 1.0510 - accuracy: 0.42 - ETA: 1s - loss: 1.0510 - accuracy: 0.42 - ETA: 1s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 0s - loss: 1.0504 - accuracy: 0.42 - ETA: 0s - loss: 1.0505 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0510 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0512 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - 9s 989us/step - loss: 1.0511 - accuracy: 0.4277 - val_loss: 1.0446 - val_accuracy: 0.4414\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.1199 - accuracy: 0.40 - ETA: 8s - loss: 1.0374 - accuracy: 0.43 - ETA: 8s - loss: 1.0439 - accuracy: 0.42 - ETA: 7s - loss: 1.0507 - accuracy: 0.42 - ETA: 7s - loss: 1.0548 - accuracy: 0.42 - ETA: 7s - loss: 1.0508 - accuracy: 0.42 - ETA: 7s - loss: 1.0503 - accuracy: 0.42 - ETA: 7s - loss: 1.0485 - accuracy: 0.43 - ETA: 7s - loss: 1.0486 - accuracy: 0.43 - ETA: 7s - loss: 1.0480 - accuracy: 0.43 - ETA: 7s - loss: 1.0482 - accuracy: 0.43 - ETA: 6s - loss: 1.0492 - accuracy: 0.43 - ETA: 7s - loss: 1.0500 - accuracy: 0.43 - ETA: 7s - loss: 1.0526 - accuracy: 0.43 - ETA: 7s - loss: 1.0615 - accuracy: 0.43 - ETA: 7s - loss: 1.0614 - accuracy: 0.43 - ETA: 7s - loss: 1.0629 - accuracy: 0.43 - ETA: 7s - loss: 1.0621 - accuracy: 0.43 - ETA: 6s - loss: 1.0609 - accuracy: 0.43 - ETA: 7s - loss: 1.0612 - accuracy: 0.43 - ETA: 7s - loss: 1.0606 - accuracy: 0.43 - ETA: 7s - loss: 1.0594 - accuracy: 0.43 - ETA: 6s - loss: 1.0599 - accuracy: 0.43 - ETA: 6s - loss: 1.0589 - accuracy: 0.43 - ETA: 6s - loss: 1.0600 - accuracy: 0.43 - ETA: 6s - loss: 1.0589 - accuracy: 0.43 - ETA: 6s - loss: 1.0590 - accuracy: 0.43 - ETA: 6s - loss: 1.0586 - accuracy: 0.43 - ETA: 6s - loss: 1.0579 - accuracy: 0.43 - ETA: 6s - loss: 1.0569 - accuracy: 0.43 - ETA: 6s - loss: 1.0561 - accuracy: 0.43 - ETA: 6s - loss: 1.0560 - accuracy: 0.43 - ETA: 6s - loss: 1.0563 - accuracy: 0.43 - ETA: 6s - loss: 1.0563 - accuracy: 0.43 - ETA: 6s - loss: 1.0574 - accuracy: 0.43 - ETA: 6s - loss: 1.0581 - accuracy: 0.42 - ETA: 6s - loss: 1.0576 - accuracy: 0.42 - ETA: 6s - loss: 1.0571 - accuracy: 0.43 - ETA: 6s - loss: 1.0566 - accuracy: 0.43 - ETA: 6s - loss: 1.0563 - accuracy: 0.43 - ETA: 5s - loss: 1.0560 - accuracy: 0.43 - ETA: 5s - loss: 1.0561 - accuracy: 0.42 - ETA: 5s - loss: 1.0562 - accuracy: 0.42 - ETA: 5s - loss: 1.0556 - accuracy: 0.42 - ETA: 5s - loss: 1.0547 - accuracy: 0.42 - ETA: 5s - loss: 1.0554 - accuracy: 0.42 - ETA: 5s - loss: 1.0556 - accuracy: 0.42 - ETA: 5s - loss: 1.0558 - accuracy: 0.42 - ETA: 5s - loss: 1.0554 - accuracy: 0.42 - ETA: 5s - loss: 1.0551 - accuracy: 0.42 - ETA: 5s - loss: 1.0553 - accuracy: 0.42 - ETA: 5s - loss: 1.0552 - accuracy: 0.42 - ETA: 5s - loss: 1.0553 - accuracy: 0.42 - ETA: 5s - loss: 1.0551 - accuracy: 0.42 - ETA: 5s - loss: 1.0547 - accuracy: 0.42 - ETA: 5s - loss: 1.0546 - accuracy: 0.42 - ETA: 5s - loss: 1.0541 - accuracy: 0.42 - ETA: 5s - loss: 1.0540 - accuracy: 0.42 - ETA: 5s - loss: 1.0540 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0539 - accuracy: 0.42 - ETA: 5s - loss: 1.0538 - accuracy: 0.42 - ETA: 5s - loss: 1.0537 - accuracy: 0.42 - ETA: 5s - loss: 1.0537 - accuracy: 0.42 - ETA: 5s - loss: 1.0536 - accuracy: 0.42 - ETA: 5s - loss: 1.0536 - accuracy: 0.42 - ETA: 5s - loss: 1.0531 - accuracy: 0.43 - ETA: 5s - loss: 1.0528 - accuracy: 0.43 - ETA: 5s - loss: 1.0526 - accuracy: 0.42 - ETA: 5s - loss: 1.0526 - accuracy: 0.43 - ETA: 5s - loss: 1.0526 - accuracy: 0.42 - ETA: 5s - loss: 1.0524 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0523 - accuracy: 0.42 - ETA: 4s - loss: 1.0525 - accuracy: 0.42 - ETA: 4s - loss: 1.0526 - accuracy: 0.42 - ETA: 4s - loss: 1.0523 - accuracy: 0.42 - ETA: 4s - loss: 1.0523 - accuracy: 0.42 - ETA: 4s - loss: 1.0522 - accuracy: 0.42 - ETA: 4s - loss: 1.0521 - accuracy: 0.42 - ETA: 4s - loss: 1.0519 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0516 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0519 - accuracy: 0.42 - ETA: 4s - loss: 1.0519 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0521 - accuracy: 0.42 - ETA: 4s - loss: 1.0530 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0525 - accuracy: 0.42 - ETA: 3s - loss: 1.0522 - accuracy: 0.42 - ETA: 3s - loss: 1.0521 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0524 - accuracy: 0.42 - ETA: 3s - loss: 1.0525 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0525 - accuracy: 0.42 - ETA: 3s - loss: 1.0526 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0528 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0526 - accuracy: 0.42 - ETA: 3s - loss: 1.0525 - accuracy: 0.42 - ETA: 3s - loss: 1.0525 - accuracy: 0.42 - ETA: 3s - loss: 1.0524 - accuracy: 0.42 - ETA: 3s - loss: 1.0524 - accuracy: 0.42 - ETA: 3s - loss: 1.0523 - accuracy: 0.42 - ETA: 3s - loss: 1.0523 - accuracy: 0.42 - ETA: 3s - loss: 1.0523 - accuracy: 0.42 - ETA: 3s - loss: 1.0522 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0519 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0522 - accuracy: 0.42 - ETA: 3s - loss: 1.0521 - accuracy: 0.42 - ETA: 3s - loss: 1.0522 - accuracy: 0.42 - ETA: 3s - loss: 1.0521 - accuracy: 0.42 - ETA: 3s - loss: 1.0521 - accuracy: 0.42 - ETA: 3s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0523 - accuracy: 0.42 - ETA: 2s - loss: 1.0521 - accuracy: 0.42 - ETA: 2s - loss: 1.0521 - accuracy: 0.42 - ETA: 2s - loss: 1.0523 - accuracy: 0.42 - ETA: 2s - loss: 1.0521 - accuracy: 0.42 - ETA: 2s - loss: 1.0520 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 2s - loss: 1.0522 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 1s - loss: 1.0523 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 1s - loss: 1.0522 - accuracy: 0.42 - ETA: 1s - loss: 1.0525 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 1s - loss: 1.0526 - accuracy: 0.42 - ETA: 1s - loss: 1.0526 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 1s - loss: 1.0525 - accuracy: 0.42 - ETA: 1s - loss: 1.0525 - accuracy: 0.42 - ETA: 1s - loss: 1.0523 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0524 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0524 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0520 - accuracy: 0.42 - ETA: 0s - loss: 1.0520 - accuracy: 0.42 - ETA: 0s - loss: 1.0520 - accuracy: 0.42 - ETA: 0s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0522 - accuracy: 0.42 - ETA: 0s - loss: 1.0523 - accuracy: 0.42 - 10s 1ms/step - loss: 1.0523 - accuracy: 0.4278 - val_loss: 1.0180 - val_accuracy: 0.4414\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.0505 - accuracy: 0.37 - ETA: 6s - loss: 1.0398 - accuracy: 0.43 - ETA: 6s - loss: 1.0326 - accuracy: 0.43 - ETA: 6s - loss: 1.0423 - accuracy: 0.42 - ETA: 8s - loss: 1.0479 - accuracy: 0.42 - ETA: 9s - loss: 1.0488 - accuracy: 0.42 - ETA: 10s - loss: 1.0462 - accuracy: 0.424 - ETA: 10s - loss: 1.0449 - accuracy: 0.426 - ETA: 10s - loss: 1.0440 - accuracy: 0.427 - ETA: 9s - loss: 1.0446 - accuracy: 0.426 - ETA: 9s - loss: 1.0443 - accuracy: 0.42 - ETA: 9s - loss: 1.0435 - accuracy: 0.42 - ETA: 9s - loss: 1.0437 - accuracy: 0.42 - ETA: 9s - loss: 1.0442 - accuracy: 0.42 - ETA: 10s - loss: 1.0444 - accuracy: 0.428 - ETA: 10s - loss: 1.0442 - accuracy: 0.430 - ETA: 9s - loss: 1.0448 - accuracy: 0.429 - ETA: 9s - loss: 1.0485 - accuracy: 0.42 - ETA: 9s - loss: 1.0478 - accuracy: 0.42 - ETA: 9s - loss: 1.0484 - accuracy: 0.42 - ETA: 8s - loss: 1.0484 - accuracy: 0.42 - ETA: 8s - loss: 1.0502 - accuracy: 0.42 - ETA: 8s - loss: 1.0502 - accuracy: 0.42 - ETA: 8s - loss: 1.0517 - accuracy: 0.42 - ETA: 7s - loss: 1.0526 - accuracy: 0.42 - ETA: 7s - loss: 1.0521 - accuracy: 0.42 - ETA: 7s - loss: 1.0516 - accuracy: 0.42 - ETA: 7s - loss: 1.0517 - accuracy: 0.42 - ETA: 7s - loss: 1.0517 - accuracy: 0.42 - ETA: 7s - loss: 1.0507 - accuracy: 0.42 - ETA: 7s - loss: 1.0511 - accuracy: 0.42 - ETA: 7s - loss: 1.0517 - accuracy: 0.42 - ETA: 7s - loss: 1.0513 - accuracy: 0.42 - ETA: 6s - loss: 1.0509 - accuracy: 0.42 - ETA: 6s - loss: 1.0496 - accuracy: 0.42 - ETA: 6s - loss: 1.0493 - accuracy: 0.42 - ETA: 6s - loss: 1.0494 - accuracy: 0.42 - ETA: 6s - loss: 1.0489 - accuracy: 0.42 - ETA: 6s - loss: 1.0507 - accuracy: 0.42 - ETA: 6s - loss: 1.0499 - accuracy: 0.42 - ETA: 6s - loss: 1.0491 - accuracy: 0.42 - ETA: 6s - loss: 1.0489 - accuracy: 0.42 - ETA: 6s - loss: 1.0486 - accuracy: 0.42 - ETA: 6s - loss: 1.0487 - accuracy: 0.42 - ETA: 6s - loss: 1.0487 - accuracy: 0.42 - ETA: 5s - loss: 1.0485 - accuracy: 0.42 - ETA: 5s - loss: 1.0481 - accuracy: 0.42 - ETA: 5s - loss: 1.0484 - accuracy: 0.42 - ETA: 5s - loss: 1.0491 - accuracy: 0.42 - ETA: 5s - loss: 1.0495 - accuracy: 0.42 - ETA: 5s - loss: 1.0493 - accuracy: 0.42 - ETA: 5s - loss: 1.0492 - accuracy: 0.42 - ETA: 5s - loss: 1.0490 - accuracy: 0.42 - ETA: 5s - loss: 1.0487 - accuracy: 0.42 - ETA: 5s - loss: 1.0495 - accuracy: 0.42 - ETA: 5s - loss: 1.0499 - accuracy: 0.42 - ETA: 5s - loss: 1.0498 - accuracy: 0.42 - ETA: 5s - loss: 1.0508 - accuracy: 0.42 - ETA: 5s - loss: 1.0510 - accuracy: 0.42 - ETA: 5s - loss: 1.0511 - accuracy: 0.42 - ETA: 4s - loss: 1.0514 - accuracy: 0.42 - ETA: 4s - loss: 1.0519 - accuracy: 0.42 - ETA: 4s - loss: 1.0521 - accuracy: 0.42 - ETA: 4s - loss: 1.0520 - accuracy: 0.42 - ETA: 4s - loss: 1.0519 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0517 - accuracy: 0.42 - ETA: 4s - loss: 1.0516 - accuracy: 0.42 - ETA: 4s - loss: 1.0520 - accuracy: 0.42 - ETA: 4s - loss: 1.0520 - accuracy: 0.42 - ETA: 4s - loss: 1.0517 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0518 - accuracy: 0.42 - ETA: 4s - loss: 1.0517 - accuracy: 0.42 - ETA: 4s - loss: 1.0517 - accuracy: 0.42 - ETA: 3s - loss: 1.0514 - accuracy: 0.42 - ETA: 3s - loss: 1.0514 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0519 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0520 - accuracy: 0.42 - ETA: 3s - loss: 1.0526 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0528 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0531 - accuracy: 0.42 - ETA: 3s - loss: 1.0529 - accuracy: 0.42 - ETA: 3s - loss: 1.0527 - accuracy: 0.42 - ETA: 3s - loss: 1.0525 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0523 - accuracy: 0.42 - ETA: 2s - loss: 1.0524 - accuracy: 0.42 - ETA: 2s - loss: 1.0523 - accuracy: 0.42 - ETA: 2s - loss: 1.0521 - accuracy: 0.42 - ETA: 2s - loss: 1.0528 - accuracy: 0.42 - ETA: 2s - loss: 1.0531 - accuracy: 0.42 - ETA: 2s - loss: 1.0533 - accuracy: 0.42 - ETA: 2s - loss: 1.0532 - accuracy: 0.42 - ETA: 2s - loss: 1.0533 - accuracy: 0.42 - ETA: 2s - loss: 1.0532 - accuracy: 0.42 - ETA: 2s - loss: 1.0530 - accuracy: 0.42 - ETA: 2s - loss: 1.0532 - accuracy: 0.42 - ETA: 2s - loss: 1.0535 - accuracy: 0.42 - ETA: 2s - loss: 1.0539 - accuracy: 0.42 - ETA: 2s - loss: 1.0541 - accuracy: 0.42 - ETA: 1s - loss: 1.0540 - accuracy: 0.42 - ETA: 1s - loss: 1.0539 - accuracy: 0.42 - ETA: 1s - loss: 1.0537 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0535 - accuracy: 0.42 - ETA: 1s - loss: 1.0531 - accuracy: 0.42 - ETA: 1s - loss: 1.0530 - accuracy: 0.42 - ETA: 1s - loss: 1.0531 - accuracy: 0.42 - ETA: 1s - loss: 1.0529 - accuracy: 0.42 - ETA: 1s - loss: 1.0527 - accuracy: 0.42 - ETA: 1s - loss: 1.0528 - accuracy: 0.42 - ETA: 1s - loss: 1.0525 - accuracy: 0.42 - ETA: 1s - loss: 1.0524 - accuracy: 0.42 - ETA: 1s - loss: 1.0522 - accuracy: 0.42 - ETA: 1s - loss: 1.0523 - accuracy: 0.42 - ETA: 1s - loss: 1.0522 - accuracy: 0.42 - ETA: 1s - loss: 1.0522 - accuracy: 0.42 - ETA: 1s - loss: 1.0522 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 1s - loss: 1.0521 - accuracy: 0.42 - ETA: 0s - loss: 1.0520 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0519 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0520 - accuracy: 0.42 - ETA: 0s - loss: 1.0519 - accuracy: 0.42 - ETA: 0s - loss: 1.0519 - accuracy: 0.42 - ETA: 0s - loss: 1.0516 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0519 - accuracy: 0.42 - ETA: 0s - loss: 1.0519 - accuracy: 0.42 - ETA: 0s - loss: 1.0520 - accuracy: 0.42 - ETA: 0s - loss: 1.0519 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0517 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0517 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - ETA: 0s - loss: 1.0518 - accuracy: 0.42 - 9s 916us/step - loss: 1.0518 - accuracy: 0.4268 - val_loss: 1.0497 - val_accuracy: 0.4105\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403/9403 [==============================] - ETA: 0s - loss: 1.2236 - accuracy: 0.43 - ETA: 6s - loss: 1.0462 - accuracy: 0.41 - ETA: 6s - loss: 1.0461 - accuracy: 0.42 - ETA: 6s - loss: 1.0330 - accuracy: 0.43 - ETA: 6s - loss: 1.0408 - accuracy: 0.43 - ETA: 6s - loss: 1.0518 - accuracy: 0.42 - ETA: 6s - loss: 1.0525 - accuracy: 0.42 - ETA: 6s - loss: 1.0545 - accuracy: 0.42 - ETA: 6s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0537 - accuracy: 0.42 - ETA: 6s - loss: 1.0533 - accuracy: 0.42 - ETA: 6s - loss: 1.0507 - accuracy: 0.42 - ETA: 5s - loss: 1.0477 - accuracy: 0.42 - ETA: 5s - loss: 1.0489 - accuracy: 0.42 - ETA: 5s - loss: 1.0479 - accuracy: 0.42 - ETA: 5s - loss: 1.0476 - accuracy: 0.42 - ETA: 5s - loss: 1.0463 - accuracy: 0.42 - ETA: 5s - loss: 1.0460 - accuracy: 0.42 - ETA: 5s - loss: 1.0465 - accuracy: 0.42 - ETA: 5s - loss: 1.0463 - accuracy: 0.42 - ETA: 5s - loss: 1.0463 - accuracy: 0.42 - ETA: 5s - loss: 1.0473 - accuracy: 0.42 - ETA: 5s - loss: 1.0479 - accuracy: 0.42 - ETA: 5s - loss: 1.0481 - accuracy: 0.42 - ETA: 5s - loss: 1.0486 - accuracy: 0.42 - ETA: 5s - loss: 1.0476 - accuracy: 0.42 - ETA: 5s - loss: 1.0483 - accuracy: 0.42 - ETA: 5s - loss: 1.0496 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0498 - accuracy: 0.42 - ETA: 5s - loss: 1.0497 - accuracy: 0.42 - ETA: 5s - loss: 1.0489 - accuracy: 0.42 - ETA: 5s - loss: 1.0490 - accuracy: 0.42 - ETA: 5s - loss: 1.0487 - accuracy: 0.42 - ETA: 5s - loss: 1.0487 - accuracy: 0.42 - ETA: 5s - loss: 1.0496 - accuracy: 0.42 - ETA: 5s - loss: 1.0494 - accuracy: 0.42 - ETA: 5s - loss: 1.0494 - accuracy: 0.42 - ETA: 5s - loss: 1.0494 - accuracy: 0.42 - ETA: 5s - loss: 1.0484 - accuracy: 0.42 - ETA: 5s - loss: 1.0485 - accuracy: 0.42 - ETA: 5s - loss: 1.0493 - accuracy: 0.42 - ETA: 5s - loss: 1.0498 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0507 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0499 - accuracy: 0.42 - ETA: 5s - loss: 1.0506 - accuracy: 0.42 - ETA: 5s - loss: 1.0506 - accuracy: 0.42 - ETA: 5s - loss: 1.0508 - accuracy: 0.42 - ETA: 5s - loss: 1.0509 - accuracy: 0.42 - ETA: 5s - loss: 1.0508 - accuracy: 0.42 - ETA: 5s - loss: 1.0507 - accuracy: 0.42 - ETA: 5s - loss: 1.0507 - accuracy: 0.42 - ETA: 5s - loss: 1.0504 - accuracy: 0.42 - ETA: 5s - loss: 1.0503 - accuracy: 0.42 - ETA: 5s - loss: 1.0499 - accuracy: 0.42 - ETA: 5s - loss: 1.0499 - accuracy: 0.42 - ETA: 5s - loss: 1.0496 - accuracy: 0.42 - ETA: 5s - loss: 1.0496 - accuracy: 0.42 - ETA: 5s - loss: 1.0494 - accuracy: 0.42 - ETA: 5s - loss: 1.0497 - accuracy: 0.42 - ETA: 5s - loss: 1.0500 - accuracy: 0.42 - ETA: 5s - loss: 1.0503 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0502 - accuracy: 0.42 - ETA: 5s - loss: 1.0500 - accuracy: 0.42 - ETA: 5s - loss: 1.0501 - accuracy: 0.42 - ETA: 5s - loss: 1.0505 - accuracy: 0.42 - ETA: 5s - loss: 1.0506 - accuracy: 0.42 - ETA: 4s - loss: 1.0504 - accuracy: 0.42 - ETA: 4s - loss: 1.0502 - accuracy: 0.42 - ETA: 4s - loss: 1.0503 - accuracy: 0.42 - ETA: 4s - loss: 1.0503 - accuracy: 0.42 - ETA: 4s - loss: 1.0501 - accuracy: 0.42 - ETA: 4s - loss: 1.0498 - accuracy: 0.42 - ETA: 4s - loss: 1.0503 - accuracy: 0.42 - ETA: 4s - loss: 1.0508 - accuracy: 0.42 - ETA: 4s - loss: 1.0507 - accuracy: 0.42 - ETA: 4s - loss: 1.0503 - accuracy: 0.42 - ETA: 4s - loss: 1.0502 - accuracy: 0.42 - ETA: 4s - loss: 1.0499 - accuracy: 0.42 - ETA: 4s - loss: 1.0496 - accuracy: 0.42 - ETA: 4s - loss: 1.0496 - accuracy: 0.42 - ETA: 4s - loss: 1.0499 - accuracy: 0.42 - ETA: 4s - loss: 1.0499 - accuracy: 0.42 - ETA: 4s - loss: 1.0495 - accuracy: 0.42 - ETA: 4s - loss: 1.0495 - accuracy: 0.42 - ETA: 4s - loss: 1.0495 - accuracy: 0.42 - ETA: 4s - loss: 1.0494 - accuracy: 0.42 - ETA: 4s - loss: 1.0492 - accuracy: 0.42 - ETA: 4s - loss: 1.0492 - accuracy: 0.42 - ETA: 3s - loss: 1.0493 - accuracy: 0.42 - ETA: 3s - loss: 1.0494 - accuracy: 0.42 - ETA: 3s - loss: 1.0492 - accuracy: 0.42 - ETA: 3s - loss: 1.0494 - accuracy: 0.42 - ETA: 3s - loss: 1.0496 - accuracy: 0.42 - ETA: 3s - loss: 1.0497 - accuracy: 0.42 - ETA: 3s - loss: 1.0497 - accuracy: 0.42 - ETA: 3s - loss: 1.0497 - accuracy: 0.42 - ETA: 3s - loss: 1.0497 - accuracy: 0.42 - ETA: 3s - loss: 1.0496 - accuracy: 0.42 - ETA: 3s - loss: 1.0496 - accuracy: 0.42 - ETA: 3s - loss: 1.0497 - accuracy: 0.42 - ETA: 3s - loss: 1.0498 - accuracy: 0.42 - ETA: 3s - loss: 1.0497 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0498 - accuracy: 0.42 - ETA: 3s - loss: 1.0499 - accuracy: 0.42 - ETA: 3s - loss: 1.0500 - accuracy: 0.42 - ETA: 3s - loss: 1.0506 - accuracy: 0.42 - ETA: 3s - loss: 1.0507 - accuracy: 0.42 - ETA: 3s - loss: 1.0508 - accuracy: 0.42 - ETA: 3s - loss: 1.0511 - accuracy: 0.42 - ETA: 3s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0506 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0505 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0505 - accuracy: 0.42 - ETA: 2s - loss: 1.0506 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0505 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0503 - accuracy: 0.42 - ETA: 2s - loss: 1.0504 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0508 - accuracy: 0.42 - ETA: 2s - loss: 1.0510 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 2s - loss: 1.0507 - accuracy: 0.42 - ETA: 2s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0509 - accuracy: 0.42 - ETA: 1s - loss: 1.0508 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0505 - accuracy: 0.42 - ETA: 1s - loss: 1.0507 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0506 - accuracy: 0.42 - ETA: 1s - loss: 1.0504 - accuracy: 0.42 - ETA: 1s - loss: 1.0504 - accuracy: 0.42 - ETA: 1s - loss: 1.0504 - accuracy: 0.42 - ETA: 1s - loss: 1.0504 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0507 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0508 - accuracy: 0.42 - ETA: 0s - loss: 1.0509 - accuracy: 0.42 - ETA: 0s - loss: 1.0511 - accuracy: 0.42 - ETA: 0s - loss: 1.0513 - accuracy: 0.42 - 10s 1ms/step - loss: 1.0513 - accuracy: 0.4269 - val_loss: 1.0184 - val_accuracy: 0.4414\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 41dbf7cf59c1b3e5d5dae6eb5984d988</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.4413926899433136</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-learning_rate: 1.0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-num_layers: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_0: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_1: 4</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_2: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_3: 7</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_4: 9</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-units_5: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units_6: 10</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py:201: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y = (y - self._y_train_mean) / self._y_train_std\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-5d7520f0158a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Search for the best parameters of the neural network using the contructed Hypberband tuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m bayesian_tuner.search(X_train.values, y_train,\n\u001b[0m\u001b[1;32m     11\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m              validation_data=(X_test.values, y_test))\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/kerastuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOPPED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# Oracle triggered exit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/kerastuner/engine/oracle.py\u001b[0m in \u001b[0;36mcreate_trial\u001b[0;34m(self, tuner_id)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_populate_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'values'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'values'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/kerastuner/tuners/bayesian.py\u001b[0m in \u001b[0;36m_populate_space\u001b[0;34m(self, trial_id)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvergenceWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# If convergence of the GPR fails, create a random trial.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# First optimize starting from theta specified in kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             optima = [(self._constrained_optimization(obj_func,\n\u001b[0m\u001b[1;32m    233\u001b[0m                                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                                                       self.kernel_.bounds))]\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_constrained_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"fmin_l_bfgs_b\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             opt_res = scipy.optimize.minimize(\n\u001b[0m\u001b[1;32m    502\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m                 bounds=bounds)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    615\u001b[0m                                   **options)\n\u001b[1;32m    616\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    618\u001b[0m                                 callback=callback, **options)\n\u001b[1;32m    619\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0miprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m    307\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    262\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                     lml, grad = self.log_marginal_likelihood(\n\u001b[0m\u001b[1;32m    225\u001b[0m                         theta, eval_gradient=True, clone_kernel=False)\n\u001b[1;32m    226\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/gaussian_process/_gpr.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcho_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Line 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;31m# Compute log-likelihood (compare line 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/scipy/linalg/decomp_cholesky.py\u001b[0m in \u001b[0;36mcho_solve\u001b[0;34m(c_and_lower, b, overwrite_b, check_finite)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_and_lower\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray_chkfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    499\u001b[0m             \"array must not contain infs or NaNs\")\n\u001b[1;32m    500\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "# Construct the BayesianOptimization tuner using the hypermodel class created\n",
    "bayesian_tuner = BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    seed=10,\n",
    "    project_name='WILDFIRE Model')\n",
    "\n",
    "# Search for the best parameters of the neural network using the contructed Hypberband tuner\n",
    "bayesian_tuner.search(X_train.values, y_train,\n",
    "             epochs=10,\n",
    "             validation_data=(X_test.values, y_test))\n",
    "\n",
    "# Get the best hyperparameters from the search\n",
    "bayesian_params = bayesian_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Build the model using the best hyperparameters\n",
    "bayesian_model = bayesian_tuner.hypermodel.build(bayesian_params)\n",
    "\n",
    "# Train the best fitting model\n",
    "bayesian_model.fit(X.values, y, epochs=15)\n",
    "\n",
    "# Check the accuracy plots\n",
    "bayesian_accuracy_df = pd.DataFrame(bayesian_model.history.history)\n",
    "\n",
    "bayesian_accuracy_df[['loss', 'accuracy']].plot()\n",
    "plt.title('Loss & Accuracy Per EPOCH For Bayesian Optimisation Model')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('Accruacy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# first we have to make sure to input data and params into the function\n",
    "def wildfire_model(x_train, y_train, x_val, y_val, params):\n",
    "\n",
    "    # next we can build the model exactly like we would normally do it\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=x_train.shape[1],\n",
    "                    activation=params['activation'],\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    # if we want to also test for number of layers and shapes, that's possible\n",
    "    hidden_layers(model, params, 1)\n",
    "   \n",
    "    # then we finish again with completely standard Keras way\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer='normal'))\n",
    "    \n",
    "    model.compile(loss=params['losses'],\n",
    "                  # here we add a regulizer normalization function from Talos\n",
    "                  optimizer=params['optimizer'](lr=lr_normalizer(params['lr'],params['optimizer'])),\n",
    "                  metrics=['acc', fmeasure])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        verbose=0)\n",
    "    \n",
    "    # finally we have to make sure that history object and model are returned\n",
    "    return history, model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
